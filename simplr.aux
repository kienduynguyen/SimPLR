\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{li2022vitdet}
\citation{nguyen2022boxer,cheng2022mask2former}
\citation{liu2021swintransformer}
\citation{li2022vitdet}
\citation{nguyen2022boxer,cheng2022mask2former}
\citation{liu2021swintransformer}
\citation{vaswani2017transformer}
\citation{liu2021swintransformer,dosovitskiy2021vit}
\citation{nicolas2020detr,zhu2021deformable,nguyen2022boxer}
\citation{wang2021maxdeeplab,zhang2021knet,cheng2022mask2former}
\citation{brown2020gpt3,devlin2019bert}
\citation{liu2021swintransformer}
\citation{fan2021mvit,wang2021pvit,heo2021rethinkingvit}
\citation{tsung2017fpn}
\citation{li2022vitdet}
\@writefile{toc}{\contentsline {title}{SimPLR\xspace  : A Simple and Plain Transformer for\unskip \ \ignorespaces  Object Detection and Segmentation}{1}{chapter.1}\protected@file@percent }
\@writefile{toc}{\authcount {1}}
\@writefile{toc}{\contentsline {author}{Anonymous ECCV 2024{} Submission}{1}{chapter.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1.1}\protected@file@percent }
\newlabel{sec:introduction}{{1}{1}{Introduction}{section.1.1}{}}
\newlabel{sec:introduction@cref}{{[section][1][]1}{[1][1][]1}}
\@writefile{brf}{\backcite{vaswani2017transformer}{{1}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{liu2021swintransformer,dosovitskiy2021vit}{{1}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{nicolas2020detr,zhu2021deformable,nguyen2022boxer}{{1}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{wang2021maxdeeplab,zhang2021knet,cheng2022mask2former}{{1}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{brown2020gpt3,devlin2019bert}{{1}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{liu2021swintransformer}{{1}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{fan2021mvit,wang2021pvit,heo2021rethinkingvit}{{1}{1}{figure.caption.2}}}
\citation{dosovitskiy2021vit}
\citation{he2022mae,bao2022beit,dehghani2023scalingvit22b}
\citation{nicolas2020detr}
\citation{dosovitskiy2021vit,li2022vitdet}
\citation{ren2015faster_rcnn,he2017maskrcnn,li2022vitdet}
\citation{zhu2021deformable,nguyen2022boxer,cheng2022mask2former}
\citation{nicolas2020detr,zhu2021deformable,nguyen2022boxer,cheng2022mask2former}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  \textbf  {Object detection architectures. Left:} The plain-backbone detector from\nobreakspace  {}\cite  {li2022vitdet} whose input (denoted in the dashed region) are multi-scale features. \textbf  {Middle:} State-of-the-art end-to-end detectors\nobreakspace  {}\cite  {nguyen2022boxer,cheng2022mask2former} utilize a hierarchical backbone (\emph  {i.e}\onedot  , Swin\nobreakspace  {}\cite  {liu2021swintransformer}) to create multi-scale inputs. \textbf  {Right:} Our simple single-scale detector following the end-to-end framework. Where existing detectors require feature pyramids to be effective, we propose a plain detector, SimPLR\xspace  , whose backbone and detection head are non-hierarchical and operate on a single-scale feature map. The plain detector, SimPLR\xspace  , achieves on par or even better performance compared to hierarchical and/or multi-scale counterparts while being more efficient. \relax }}{2}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:compare}{{1}{2}{\textbf {Object detection architectures. Left:} The plain-backbone detector from~\cite {li2022vitdet} whose input (denoted in the dashed region) are multi-scale features. \textbf {Middle:} State-of-the-art end-to-end detectors~\cite {nguyen2022boxer,cheng2022mask2former} utilize a hierarchical backbone (\ie , Swin~\cite {liu2021swintransformer}) to create multi-scale inputs. \textbf {Right:} Our simple single-scale detector following the end-to-end framework. Where existing detectors require feature pyramids to be effective, we propose a plain detector, \ours , whose backbone and detection head are non-hierarchical and operate on a single-scale feature map. The plain detector, \ours , achieves on par or even better performance compared to hierarchical and/or multi-scale counterparts while being more efficient. \relax }{figure.caption.2}{}}
\newlabel{fig:compare@cref}{{[figure][1][]1}{[1][1][]2}}
\@writefile{brf}{\backcite{li2022vitdet}{{2}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{nguyen2022boxer,cheng2022mask2former}{{2}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{liu2021swintransformer}{{2}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{tsung2017fpn}{{2}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{li2022vitdet}{{2}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{dosovitskiy2021vit}{{2}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{he2022mae,bao2022beit,dehghani2023scalingvit22b}{{2}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{nicolas2020detr}{{2}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{dosovitskiy2021vit,li2022vitdet}{{2}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{ren2015faster_rcnn,he2017maskrcnn,li2022vitdet}{{2}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{zhu2021deformable,nguyen2022boxer,cheng2022mask2former}{{2}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{nicolas2020detr,zhu2021deformable,nguyen2022boxer,cheng2022mask2former}{{2}{1}{figure.caption.2}}}
\citation{li2022vitdet}
\citation{cheng2022mask2former}
\citation{he2022mae}
\citation{peng2022beitv2}
\citation{girshick2014rcnn}
\citation{simonyan2015vgg,xie2017resnext,kaiming2016resnet,huang2017densenet}
\citation{lecun95convolutional}
\citation{deng2009imagenet}
\citation{brown2020gpt3,devlin2019bert}
\citation{chen2020igpt,dosovitskiy2021vit,liu2021swintransformer}
\citation{dosovitskiy2021vit}
\citation{chen2020simclr,he2022mae}
\citation{nicolas2020detr}
\citation{dong2021solq,wang2021maxdeeplab,nguyen2022boxer}
\citation{cheng2021maskformer}
\citation{zhang2021knet}
\citation{cheng2022mask2former}
\citation{yu2022kmmask}
\citation{zhang2023dino,li2023maskdino}
\citation{chen2022uvit}
\citation{li2022vitdet}
\citation{he2022mae}
\@writefile{brf}{\backcite{li2022vitdet}{{3}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{cheng2022mask2former}{{3}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{he2022mae}{{3}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{peng2022beitv2}{{3}{1}{figure.caption.2}}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{3}{section.1.2}\protected@file@percent }
\newlabel{sec:related_work}{{2}{3}{Related Work}{section.1.2}{}}
\newlabel{sec:related_work@cref}{{[section][2][]2}{[1][3][]3}}
\@writefile{brf}{\backcite{girshick2014rcnn}{{3}{2}{section.1.2}}}
\@writefile{brf}{\backcite{simonyan2015vgg,xie2017resnext,kaiming2016resnet,huang2017densenet}{{3}{2}{section.1.2}}}
\@writefile{brf}{\backcite{lecun95convolutional}{{3}{2}{section.1.2}}}
\@writefile{brf}{\backcite{deng2009imagenet}{{3}{2}{section.1.2}}}
\@writefile{brf}{\backcite{brown2020gpt3,devlin2019bert}{{3}{2}{section.1.2}}}
\@writefile{brf}{\backcite{chen2020igpt,dosovitskiy2021vit,liu2021swintransformer}{{3}{2}{section.1.2}}}
\@writefile{brf}{\backcite{dosovitskiy2021vit}{{3}{2}{section.1.2}}}
\@writefile{brf}{\backcite{chen2020simclr,he2022mae}{{3}{2}{section.1.2}}}
\@writefile{brf}{\backcite{nicolas2020detr}{{3}{2}{section.1.2}}}
\@writefile{brf}{\backcite{dong2021solq,wang2021maxdeeplab,nguyen2022boxer}{{3}{2}{section.1.2}}}
\@writefile{brf}{\backcite{cheng2021maskformer}{{3}{2}{section.1.2}}}
\@writefile{brf}{\backcite{zhang2021knet}{{3}{2}{section.1.2}}}
\@writefile{brf}{\backcite{cheng2022mask2former}{{3}{2}{section.1.2}}}
\@writefile{brf}{\backcite{yu2022kmmask}{{3}{2}{section.1.2}}}
\@writefile{brf}{\backcite{zhang2023dino,li2023maskdino}{{3}{2}{section.1.2}}}
\citation{lin2023plaindetr}
\citation{jia2023hybridmatching}
\citation{zhu2021deformable,nguyen2022boxer}
\citation{wei2016ssd,tsung2017fpn,zhu2021deformable}
\citation{li2022vitdet,chen2022uvit}
\citation{vaswani2017transformer}
\citation{nguyen2022boxer}
\citation{zhu2021deformable,li2022vitdet,nguyen2022boxer}
\citation{nguyen2022boxer}
\@writefile{brf}{\backcite{chen2022uvit}{{4}{2}{section.1.2}}}
\@writefile{brf}{\backcite{li2022vitdet}{{4}{2}{section.1.2}}}
\@writefile{brf}{\backcite{he2022mae}{{4}{2}{section.1.2}}}
\@writefile{brf}{\backcite{lin2023plaindetr}{{4}{2}{section.1.2}}}
\@writefile{brf}{\backcite{jia2023hybridmatching}{{4}{2}{section.1.2}}}
\@writefile{brf}{\backcite{zhu2021deformable,nguyen2022boxer}{{4}{2}{section.1.2}}}
\@writefile{toc}{\contentsline {section}{\numberline {3}SimPLR\xspace  : A Simple and Plain Detector}{4}{section.1.3}\protected@file@percent }
\newlabel{sec:simplr}{{3}{4}{\ours : A Simple and Plain Detector}{section.1.3}{}}
\newlabel{sec:simplr@cref}{{[section][3][]3}{[1][4][]4}}
\@writefile{brf}{\backcite{wei2016ssd,tsung2017fpn,zhu2021deformable}{{4}{3}{section.1.3}}}
\@writefile{brf}{\backcite{li2022vitdet,chen2022uvit}{{4}{3}{section.1.3}}}
\@writefile{brf}{\backcite{vaswani2017transformer}{{4}{3}{section.1.3}}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{4}{3}{section.1.3}}}
\citation{nguyen2022boxer}
\citation{zhu2021deformable}
\citation{zhu2021deformable}
\citation{nguyen2022boxer}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Background}{5}{subsection.1.3.1}\protected@file@percent }
\@writefile{brf}{\backcite{zhu2021deformable,li2022vitdet,nguyen2022boxer}{{5}{3.1}{subsection.1.3.1}}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{5}{3.1}{subsection.1.3.1}}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{5}{3.1}{subsection.1.3.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Scale-aware attention}{5}{subsection.1.3.2}\protected@file@percent }
\@writefile{brf}{\backcite{zhu2021deformable}{{5}{3.2}{subsection.1.3.2}}}
\@writefile{brf}{\backcite{zhu2021deformable}{{5}{3.2}{subsection.1.3.2}}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{5}{3.2}{subsection.1.3.2}}}
\citation{kirillov2019panoptic}
\citation{cheng2022mask2former}
\citation{nguyen2022boxer}
\citation{nguyen2022boxer}
\citation{nguyen2022boxer}
\citation{nguyen2022boxer}
\citation{nguyen2022boxer}
\citation{nguyen2022boxer}
\citation{nguyen2022boxer}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Object Representation for Panoptic Segmentation}{6}{subsection.1.3.3}\protected@file@percent }
\@writefile{brf}{\backcite{kirillov2019panoptic}{{6}{3.3}{subsection.1.3.3}}}
\@writefile{brf}{\backcite{cheng2022mask2former}{{6}{3.3}{subsection.1.3.3}}}
\citation{cheng2022mask2former}
\citation{nguyen2022boxer}
\citation{li2022vitdet}
\citation{li2022vitdet}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  \textbf  {Masked Instance-Attention. Left:} The box-attention\nobreakspace  {}\cite  {nguyen2022boxer} which samples $2\times 2$ grid features in the region of interest. \textbf  {Right:} Our masked instance-attention for dense grid sampling. The $2\times 2$ attention scores are denoted in four colours and the masked attention scores are in white.\relax }}{7}{figure.caption.3}\protected@file@percent }
\newlabel{fig:masked_instance_attn}{{2}{7}{\textbf {Masked Instance-Attention. Left:} The box-attention~\cite {nguyen2022boxer} which samples $2\times 2$ grid features in the region of interest. \textbf {Right:} Our masked instance-attention for dense grid sampling. The $2\times 2$ attention scores are denoted in four colours and the masked attention scores are in white.\relax }{figure.caption.3}{}}
\newlabel{fig:masked_instance_attn@cref}{{[figure][2][]2}{[1][6][]7}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{7}{2}{figure.caption.3}}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{7}{3.3}{figure.caption.3}}}
\@writefile{brf}{\backcite{cheng2022mask2former}{{7}{3.3}{figure.caption.3}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Network Architecture}{7}{subsection.1.3.4}\protected@file@percent }
\@writefile{brf}{\backcite{nguyen2022boxer}{{7}{3.4}{subsection.1.3.4}}}
\@writefile{brf}{\backcite{li2022vitdet}{{7}{3.4}{subsection.1.3.4}}}
\citation{he2022mae}
\citation{peng2022beitv2}
\citation{he2022mae,zhai2022scalingvit,dehghani2023scalingvit22b}
\citation{lin2014mscoco}
\citation{he2022mae}
\citation{nguyen2022boxer}
\citation{loshchilov2019adamw}
\citation{dosovitskiy2021vit}
\citation{shiasi2021lsjitter}
\citation{zhang2023dino}
\citation{nguyen2022boxer}
\citation{li2022vitdet}
\citation{lin2023plaindetr}
\citation{lin2023plaindetr}
\citation{nguyen2022boxer}
\citation{li2022vitdet}
\citation{lin2023plaindetr}
\citation{li2022vitdet}
\citation{li2022vitdet}
\citation{nguyen2022boxer}
\citation{nguyen2022boxer}
\citation{zhu2021deformable}
\citation{lin2023plaindetr}
\@writefile{brf}{\backcite{li2022vitdet}{{8}{3.4}{subsection.1.3.4}}}
\@writefile{brf}{\backcite{he2022mae}{{8}{3.4}{subsection.1.3.4}}}
\@writefile{brf}{\backcite{peng2022beitv2}{{8}{3.4}{subsection.1.3.4}}}
\@writefile{brf}{\backcite{he2022mae,zhai2022scalingvit,dehghani2023scalingvit22b}{{8}{3.4}{subsection.1.3.4}}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{8}{section.1.4}\protected@file@percent }
\newlabel{sec:experiments}{{4}{8}{Experiments}{section.1.4}{}}
\newlabel{sec:experiments@cref}{{[section][4][]4}{[1][8][]8}}
\@writefile{brf}{\backcite{lin2014mscoco}{{8}{4}{section.1.4}}}
\@writefile{brf}{\backcite{he2022mae}{{8}{4}{section.1.4}}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{8}{4}{section.1.4}}}
\@writefile{brf}{\backcite{loshchilov2019adamw}{{8}{4}{section.1.4}}}
\@writefile{brf}{\backcite{dosovitskiy2021vit}{{8}{4}{section.1.4}}}
\@writefile{brf}{\backcite{shiasi2021lsjitter}{{8}{4}{section.1.4}}}
\@writefile{brf}{\backcite{zhang2023dino}{{8}{4}{section.1.4}}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{8}{4}{section.1.4}}}
\@writefile{brf}{\backcite{li2022vitdet}{{8}{4}{section.1.4}}}
\@writefile{brf}{\backcite{zhu2021deformable}{{8}{4}{table.caption.5}}}
\@writefile{brf}{\backcite{lin2023plaindetr}{{8}{4}{table.caption.5}}}
\citation{nguyen2022boxer}
\@writefile{brf}{\backcite{lin2023plaindetr}{{9}{\caption@xref {??}{ on input line 24}}{table.caption.4}}}
\@writefile{brf}{\backcite{lin2023plaindetr}{{9}{\caption@xref {??}{ on input line 25}}{table.caption.4}}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{9}{\caption@xref {??}{ on input line 27}}{table.caption.4}}}
\@writefile{brf}{\backcite{li2022vitdet}{{9}{\caption@xref {??}{ on input line 28}}{table.caption.4}}}
\@writefile{brf}{\backcite{lin2023plaindetr}{{9}{\caption@xref {??}{ on input line 31}}{table.caption.4}}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \textbf  {SimPLR\xspace  is an effective plain detector.} All methods use ViT-B as backbone. Methods that take feature pyramids as input employ SimpleFPN with ViT from \cite  {li2022vitdet}. Our plain detector, SimPLR\xspace  , shows competitive performance compared to multi-scale alternatives, while being faster during inference.\relax }}{9}{table.caption.4}\protected@file@percent }
\newlabel{tab:compare}{{1}{9}{\textbf {\ours is an effective plain detector.} All methods use ViT-B as backbone. Methods that take feature pyramids as input employ SimpleFPN with ViT from \cite {li2022vitdet}. Our plain detector, \ours , shows competitive performance compared to multi-scale alternatives, while being faster during inference.\relax }{table.caption.4}{}}
\newlabel{tab:compare@cref}{{[table][1][]1}{[1][8][]9}}
\@writefile{brf}{\backcite{li2022vitdet}{{9}{1}{table.caption.4}}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{9}{4}{table.caption.5}}}
\citation{touvron2022deit3}
\citation{touvron2021deit}
\citation{he2022mae}
\citation{peng2022beitv2}
\citation{li2022vitdet}
\newlabel{tab:strat}{{2a}{10}{\textbf {Scale-aware attention.}\caption@thelabel \relax }{table.caption.5}{}}
\newlabel{tab:strat@cref}{{[subtable][1][2]2a}{[1][8][]10}}
\newlabel{sub@tab:strat}{{a}{10}{\textbf {Scale-aware attention.}\caption@thelabel \relax }{table.caption.5}{}}
\newlabel{sub@tab:strat@cref}{{[subtable][1][2]2a}{[1][8][]10}}
\newlabel{tab:window_size}{{2b}{10}{\textbf {Window size.}\caption@thelabel \relax }{table.caption.5}{}}
\newlabel{tab:window_size@cref}{{[subtable][2][2]2b}{[1][8][]10}}
\newlabel{sub@tab:window_size}{{b}{10}{\textbf {Window size.}\caption@thelabel \relax }{table.caption.5}{}}
\newlabel{sub@tab:window_size@cref}{{[subtable][2][2]2b}{[1][8][]10}}
\newlabel{tab:num_scale}{{2c}{10}{\textbf {Number of window scales}.\caption@thelabel \relax }{table.caption.5}{}}
\newlabel{tab:num_scale@cref}{{[subtable][3][2]2c}{[1][8][]10}}
\newlabel{sub@tab:num_scale}{{c}{10}{\textbf {Number of window scales}.\caption@thelabel \relax }{table.caption.5}{}}
\newlabel{sub@tab:num_scale@cref}{{[subtable][3][2]2c}{[1][8][]10}}
\newlabel{tab:feat_scale}{{2d}{10}{\textbf {Scales of input features.}\caption@thelabel \relax }{table.caption.5}{}}
\newlabel{tab:feat_scale@cref}{{[subtable][4][2]2d}{[1][8][]10}}
\newlabel{sub@tab:feat_scale}{{d}{10}{\textbf {Scales of input features.}\caption@thelabel \relax }{table.caption.5}{}}
\newlabel{sub@tab:feat_scale@cref}{{[subtable][4][2]2d}{[1][8][]10}}
\newlabel{fig:attn_vis}{{2e}{10}{Visualization of scale distribution learnt in \textbf {multi-head adaptive-scale attention} of object proposals. Objects are classified into \emph {small}, \emph {medium}, and \emph {large} based on their area.\caption@thelabel \relax }{table.caption.5}{}}
\newlabel{fig:attn_vis@cref}{{[subtable][5][2]2e}{[1][8][]10}}
\newlabel{sub@fig:attn_vis}{{e}{10}{Visualization of scale distribution learnt in \textbf {multi-head adaptive-scale attention} of object proposals. Objects are classified into \emph {small}, \emph {medium}, and \emph {large} based on their area.\caption@thelabel \relax }{table.caption.5}{}}
\newlabel{sub@fig:attn_vis@cref}{{[subtable][5][2]2e}{[1][8][]10}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces  \textbf  {Ablation of scale-aware attention} in SimPLR\xspace  using a plain ViT backbone on COCO\nobreakspace  {}{\texttt  {val}}\xspace  . \textbf  {Table (a-d):} Compared to the na\"ive baseline, which employs BoxeR and box-attention \cite  {nguyen2022boxer} with \textit  {single-scale} features, our plain detector, SimPLR\xspace  , with scale-aware attention improves the performance consistently for all settings, default setting highlighted. \textbf  {Figure e:} our adaptive-scale attention captures different scale distribution in its attention heads based on the context of query vectors. Specifically, queries of \emph  {small} objects tends to focus on reference windows of small scales (\emph  {i.e}\onedot  , mainly \(32\times 32\)), while query vectors of \emph  {medium} and \emph  {large} objects distribute more attention computation into larger reference windows. All experiments are with $5\times $ schedule.\relax }}{10}{table.caption.5}\protected@file@percent }
\newlabel{tab:det_ablation}{{2}{10}{\textbf {Ablation of scale-aware attention} in \ours using a plain ViT backbone on COCO~\val . \textbf {Table (a-d):} Compared to the na\"ive baseline, which employs BoxeR and box-attention \cite {nguyen2022boxer} with \textit {single-scale} features, our plain detector, \ours , with scale-aware attention improves the performance consistently for all settings, default setting highlighted. \textbf {Figure e:} our adaptive-scale attention captures different scale distribution in its attention heads based on the context of query vectors. Specifically, queries of \emph {small} objects tends to focus on reference windows of small scales (\ie , mainly \(32\times 32\)), while query vectors of \emph {medium} and \emph {large} objects distribute more attention computation into larger reference windows. All experiments are with $5\times $ schedule.\relax }{table.caption.5}{}}
\newlabel{tab:det_ablation@cref}{{[table][2][]2}{[1][8][]10}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{10}{2}{table.caption.5}}}
\@writefile{brf}{\backcite{touvron2022deit3}{{10}{4}{table.caption.5}}}
\@writefile{brf}{\backcite{touvron2021deit}{{10}{4}{table.caption.5}}}
\@writefile{brf}{\backcite{he2022mae}{{10}{4}{table.caption.5}}}
\@writefile{brf}{\backcite{peng2022beitv2}{{10}{4}{table.caption.5}}}
\citation{nguyen2022boxer}
\citation{nguyen2022boxer}
\citation{lin2014mscoco}
\citation{lin2014mscoco}
\citation{he2022mae}
\citation{peng2022beitv2}
\citation{cheng2022mask2former}
\citation{liu2021swintransformer}
\citation{fan2021mvit}
\citation{liu2021swintransformer}
\citation{cheng2022mask2former}
\citation{fan2021mvit}
\citation{li2022vitdet}
\citation{li2022vitdet}
\citation{li2022vitdet}
\citation{cheng2021maskformer}
\citation{cheng2022mask2former}
\citation{cheng2022mask2former}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces  \textbf  {Ablation on scaling with more pre-training data and strategies} of SimPLR\xspace  with the plain ViT-B backbone evaluated on COCO object detection and instance segmentation. We compare the plain backbone pre-trained using supervised methods (\emph  {top} row) \emph  {vs}\onedot  self-supervised methods (\emph  {bottom} row) with different sizes of pre-training dataset (ImageNet-1K \emph  {vs}\onedot  ImageNet-21K). Here, we use the $5\times $ schedule as in \cite  {nguyen2022boxer}. It can be seen that SimPLR\xspace  with the plain ViT backbone benefits with more pre-training data (\emph  {e.g}\onedot  , ImageNet-1K \emph  {vs}\onedot  ImageNet-21K) and better pre-training approaches (supervised learning \emph  {vs}\onedot  self-supervised learning). \relax }}{11}{table.caption.6}\protected@file@percent }
\newlabel{tab:pretrain}{{3}{11}{\textbf {Ablation on scaling with more pre-training data and strategies} of \ours with the plain ViT-B backbone evaluated on COCO object detection and instance segmentation. We compare the plain backbone pre-trained using supervised methods (\emph {top} row) \vs self-supervised methods (\emph {bottom} row) with different sizes of pre-training dataset (ImageNet-1K \vs ImageNet-21K). Here, we use the $5\times $ schedule as in \cite {nguyen2022boxer}. It can be seen that \ours with the plain ViT backbone benefits with more pre-training data (\eg , ImageNet-1K \vs ImageNet-21K) and better pre-training approaches (supervised learning \vs self-supervised learning). \relax }{table.caption.6}{}}
\newlabel{tab:pretrain@cref}{{[table][3][]3}{[1][11][]11}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{11}{3}{table.caption.6}}}
\@writefile{brf}{\backcite{li2022vitdet}{{11}{4}{table.caption.5}}}
\@writefile{brf}{\backcite{he2022mae}{{11}{4}{figure.caption.7}}}
\@writefile{brf}{\backcite{peng2022beitv2}{{11}{4}{figure.caption.7}}}
\@writefile{brf}{\backcite{cheng2022mask2former}{{11}{4}{figure.caption.7}}}
\@writefile{brf}{\backcite{liu2021swintransformer}{{11}{4}{figure.caption.7}}}
\@writefile{brf}{\backcite{fan2021mvit}{{11}{4}{figure.caption.7}}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Qualitative results} for object detection and panoptic segmentation on the COCO\nobreakspace  {}\cite  {lin2014mscoco} 2017 {\texttt  {val}}\xspace  set generated by SimPLR\xspace  . Note that SimPLR\xspace  gives good predictions on \emph  {small} objects.\relax }}{11}{figure.caption.7}\protected@file@percent }
\newlabel{fig:vis}{{3}{11}{\textbf {Qualitative results} for object detection and panoptic segmentation on the COCO~\cite {lin2014mscoco} 2017 \val set generated by \ours . Note that \ours gives good predictions on \emph {small} objects.\relax }{figure.caption.7}{}}
\newlabel{fig:vis@cref}{{[figure][3][]3}{[1][11][]11}}
\@writefile{brf}{\backcite{lin2014mscoco}{{11}{3}{figure.caption.7}}}
\bibstyle{splncs04}
\bibdata{simplr}
\bibcite{bao2022beit}{1}
\bibcite{brown2020gpt3}{2}
\@writefile{brf}{\backcite{liu2021swintransformer}{{12}{\caption@xref {??}{ on input line 258}}{table.caption.8}}}
\@writefile{brf}{\backcite{cheng2022mask2former}{{12}{\caption@xref {??}{ on input line 259}}{table.caption.8}}}
\@writefile{brf}{\backcite{fan2021mvit}{{12}{\caption@xref {??}{ on input line 260}}{table.caption.8}}}
\@writefile{brf}{\backcite{li2022vitdet}{{12}{\caption@xref {??}{ on input line 261}}{table.caption.8}}}
\@writefile{brf}{\backcite{li2022vitdet}{{12}{\caption@xref {??}{ on input line 266}}{table.caption.8}}}
\@writefile{brf}{\backcite{li2022vitdet}{{12}{\caption@xref {??}{ on input line 267}}{table.caption.8}}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces \textbf  {State-of-the-art comparison and scaling behavior for object detection and instance segmentation.} We compare methods using feature pyramids \emph  {vs}\onedot  plain features on COCO {\texttt  {val}}\xspace  (n/a: entry is not available). Backbones with MAE pre-trained on ImageNet-1K while others pre-trained on ImageNet-21K. With only single-scale features, SimPLR\xspace  shows strong performance compared to multi-scale detectors including transformer-based detectors like Mask2Former, while being $\sim 2\times $ faster.\relax }}{12}{table.caption.8}\protected@file@percent }
\newlabel{tab:det_main}{{4}{12}{\textbf {State-of-the-art comparison and scaling behavior for object detection and instance segmentation.} We compare methods using feature pyramids \vs plain features on COCO \val (n/a: entry is not available). Backbones with MAE pre-trained on ImageNet-1K while others pre-trained on ImageNet-21K. With only single-scale features, \ours shows strong performance compared to multi-scale detectors including transformer-based detectors like Mask2Former, while being $\sim 2\times $ faster.\relax }{table.caption.8}{}}
\newlabel{tab:det_main@cref}{{[table][4][]4}{[1][11][]12}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{12}{section.1.5}\protected@file@percent }
\newlabel{sec:conclusion}{{5}{12}{Conclusion}{section.1.5}{}}
\newlabel{sec:conclusion@cref}{{[section][5][]5}{[1][12][]12}}
\bibcite{nicolas2020detr}{3}
\bibcite{chen2020igpt}{4}
\bibcite{chen2020simclr}{5}
\bibcite{chen2022uvit}{6}
\bibcite{cheng2022mask2former}{7}
\bibcite{cheng2021maskformer}{8}
\bibcite{dehghani2023scalingvit22b}{9}
\bibcite{deng2009imagenet}{10}
\bibcite{devlin2019bert}{11}
\bibcite{dong2021solq}{12}
\bibcite{dosovitskiy2021vit}{13}
\bibcite{fan2021mvit}{14}
\bibcite{shiasi2021lsjitter}{15}
\bibcite{girshick2014rcnn}{16}
\@writefile{brf}{\backcite{cheng2021maskformer}{{13}{\caption@xref {??}{ on input line 298}}{table.caption.9}}}
\@writefile{brf}{\backcite{cheng2022mask2former}{{13}{\caption@xref {??}{ on input line 299}}{table.caption.9}}}
\@writefile{brf}{\backcite{cheng2022mask2former}{{13}{\caption@xref {??}{ on input line 301}}{table.caption.9}}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces \textbf  {State-of-the-art comparison and scaling behavior for panoptic segmentation.} We compare between methods using feature pyramids (\emph  {top} row) \emph  {vs}\onedot  single-scale (\emph  {bottom} row) on COCO\nobreakspace  {}{\texttt  {val}}\xspace  . SimPLR\xspace  with single-scale input shows better results when scaling to larger backbones, while being $\sim 2\times $ faster compared to Mask2Former.\relax }}{13}{table.caption.9}\protected@file@percent }
\newlabel{tab:panoptic}{{5}{13}{\textbf {State-of-the-art comparison and scaling behavior for panoptic segmentation.} We compare between methods using feature pyramids (\emph {top} row) \vs single-scale (\emph {bottom} row) on COCO~\val . \ours with single-scale input shows better results when scaling to larger backbones, while being $\sim 2\times $ faster compared to Mask2Former.\relax }{table.caption.9}{}}
\newlabel{tab:panoptic@cref}{{[table][5][]5}{[1][11][]13}}
\bibcite{he2022mae}{17}
\bibcite{he2017maskrcnn}{18}
\bibcite{kaiming2016resnet}{19}
\bibcite{heo2021rethinkingvit}{20}
\bibcite{huang2017densenet}{21}
\bibcite{jia2023hybridmatching}{22}
\bibcite{kirillov2019panoptic}{23}
\bibcite{lecun95convolutional}{24}
\bibcite{li2023maskdino}{25}
\bibcite{li2022vitdet}{26}
\bibcite{tsung2017fpn}{27}
\bibcite{lin2014mscoco}{28}
\bibcite{lin2023plaindetr}{29}
\bibcite{wei2016ssd}{30}
\bibcite{liu2021swintransformer}{31}
\bibcite{loshchilov2019adamw}{32}
\bibcite{nguyen2022boxer}{33}
\bibcite{peng2022beitv2}{34}
\bibcite{ren2015faster_rcnn}{35}
\bibcite{simonyan2015vgg}{36}
\bibcite{touvron2021deit}{37}
\bibcite{touvron2022deit3}{38}
\bibcite{vaswani2017transformer}{39}
\bibcite{wang2021maxdeeplab}{40}
\bibcite{wang2021pvit}{41}
\bibcite{xie2017resnext}{42}
\bibcite{yu2022kmmask}{43}
\bibcite{zhai2022scalingvit}{44}
\bibcite{zhang2023dino}{45}
\bibcite{zhang2021knet}{46}
\bibcite{zhu2021deformable}{47}
\gdef \@abspage@last{15}
