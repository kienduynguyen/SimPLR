\section{Related Work}
\label{sec:related_work}

\boldparagraph{Backbones for object detection.} Inspired by R-CNN~\cite{girshick2014rcnn}, modern object detection methods utilize a task-specific head on top of a pre-trained backbone. Initially, object detectors~\cite{simonyan2015vgg,xie2017resnext,kaiming2016resnet,huang2017densenet} were dominated by convolutional neural network (CNN) backbones~\cite{lecun95convolutional} pre-trained on ImageNet~\cite{deng2009imagenet}. With the success of the transformer in learning from large-scale text data~\cite{brown2020gpt3,devlin2019bert}, many studies have explored the transformer for computer vision~\cite{chen2020igpt,dosovitskiy2021vit,liu2021swintransformer}. Most recently, the Vision Transformer (ViT)~\cite{dosovitskiy2021vit} with a simple design demonstrated the capability of learning meaningful representation for visual recognition. By removing the need for labels, methods with self-supervised learning have emerged as an even more powerful solution for pre-training general vision representations~\cite{chen2020simclr,he2022mae}. We show through experiments that \ours can take advantages of the significant progress in representation learning and scaling of ViTs.

% the pre-trained ViT when combined with our proposed encoder-decoder delivers strong performance in detection and segmentation while removing non-trivial adaptations in ViT for feature pyramids.

\boldparagraph{End-to-end detection and segmentation.} The end-to-end framework for object detection proposed in DETR~\cite{nicolas2020detr} aims to remove the need for many hand-crafted modules. This is made possible by adopting Transformer as the detection head to directly give the prediction. Follow-up works~\cite{dong2021solq,wang2021maxdeeplab,nguyen2022boxer} extended the transformer-based head in end-to-end frameworks for instance segmentation, and panoptic segmentation. This inspired MaskFormer~\cite{cheng2021maskformer} and K-Net~\cite{zhang2021knet} to unify segmentation tasks with a class-agnostic mask prediction. Pointing out that MaskFormer and K-Net lag behind specialized architectures, Cheng \etal~~\cite{cheng2022mask2former} introduce Mask2Former, reaching strong performance on segmentation tasks. Yu \etal~\cite{yu2022kmmask} replace self-attention with $k$-means clustering, boosting the effectiveness of the network. Another direction is to improve the object query in the decoder via a denoising process~\cite{zhang2023dino,li2023maskdino}. While simplifying the detection and segmentation framework, these architectures still require an hierarchical backbone along with feature pyramids. The use of feature pyramids increases the sequence length of the input to the transformer-based detection head, making the detector less efficient. In this work, we enable a plain detector by removing hierarchical and multi-scale constraints.

% A universal architecture solves multiple vision tasks without many architectural changes. This is made possible by the introduction of transformers for object detection~\cite{nicolas2020detr}. Follow-up works~\cite{dong2021solq,wang2021maxdeeplab,nguyen2022boxer} extended the end-to-end detection framework to object detection, instance segmentation, and panoptic segmentation. This inspired MaskFormer~\cite{cheng2021maskformer} and K-Net~\cite{zhang2021knet} to unify segmentation tasks with a class-agnostic mask prediction. Pointing out that MaskFormer and K-Net still lag behind specialized architectures,~\cite{cheng2022mask2former} introduce Mask2Former, which outperforms specialized architectures on instance, semantic, and panoptic segmentation. ~\cite{yu2022kmmask} replace self-attention with $k$-means clustering, further boosting the effectiveness of the network. Another direction is to improve the object query in the decoder via a denoising process~\cite{zhang2023dino,li2023maskdino}. Similar to convolution-based detection heads, these approaches utilize multi-scale feature maps from a hierarchical backbone. In this work, we remove the multi-scale feature map constraint and enable \ours on detection and segmentation using a \emph{single-scale} feature map.

\boldparagraph{Plain detectors.} Following the goal of less inductive biases in the architecture, recent studies focus on the non-hierarchical and single-scale detector. Motivated by the success of ViT, a line of research considers plain-backbone detectors that replace the hierarchical backbone with a ViT. Initially, Chen \etal~\cite{chen2022uvit} present UViT as a plain detector that contains a ViT backbone and a single-scale convolutional detection head. Since the backbone architecture is modified \textit{during pre-training} to adopt the progressive window attention, UViT is unable to take the advantages of existing pre-training approaches with ViTs. ViTDet~\cite{li2022vitdet} tackles this problem with simple adaptations of the ViT backbone \textit{during fine-tuning}. These simple modifications allow ViTDet to benefit directly from recent self-supervised learning with ViTs (\ie, MAE~\cite{he2022mae}), resulting in strong results when scaling to larger models. Despite enabling plain-backbone detectors, feature pyramids are still an important factor in ViTDet to detect object at various scales.

Concurrent to our work, Lin \etal~\cite{lin2023plaindetr} introduce the transformer-based PlainDETR detector, which also removes the multi-scale input. However, it still relies on multi-scale features to generate the object proposals for its decoder. In the decoder, PlainDETR also uses hybrid matching~\cite{jia2023hybridmatching} to strengthen its prediction, while our decoder preserves a simple design as in~\cite{zhu2021deformable,nguyen2022boxer}. We believe to be the first to remove
the hierarchical and multi-scale constraints which appear in the backbone \textit{and} the input of the transformer encoder for \textit{both} detection and segmentation tasks. Our proposed scale-aware attention can further plug into current end-to-end frameworks without significant architectural changes. %, resulting in a plain detector (\ours).

% Instead, we propose to learn scale information in sparse attention mechanism within our detector (\ours). The scale-aware attention mechanism allows \ours to maintain the single-scale features in both proposal generation and final prediction, yielding a plain detector. We show that \ours demonstrates compelling scaling behavior while being efficient due to the use of sparse attention mechanism.

% Most recently,~\cite{lin2023plaindetr} suggests to remove the use of multi-scale input by designing a transformer-based detection head 

% that a transformer-based detection head with self-attention and box-to-pixel biases gives competitive results on single-scale input from . However, 

% A key challenge in object detection is to detect objects within an image across multiple scales. Early works tackled this problem by applying a CNN detector with a sliding window strategy on an image pyramid~\cite{sermanet2014overfeat} or with generated region proposals~\cite{uijlings2013selective} to extract each scale-normalized region from the input image~\cite{girshick2014rcnn}.
% To reduce computational costs, Faster R-CNN~\cite{ren2015faster_rcnn} generates object proposals on the feature map using a region proposal network. As deeper features of CNNs tends to capture more high-level information at the expense of fine-grained details for small objects, SSD~\cite{wei2016ssd} predicts objects from multiple layers of the feature hierarchy. The Feature Pyramid Network~\cite{tsung2017fpn} further improves the creation of multi-scale feature maps with top-down fusion and lateral connections. With the recent evidence that non-hierarchical transformer architectures (\ie, ViTs) are able to learn convolution-like behaviour through image data (\ie, translation-equivariance), the necessity of multi-scale feature maps becomes questionable. 
% In this study, we demonstrate that a plain ViT backbone along with a single-scale encoder-decoder is able to detect multi-scale objects without the need for feature pyramids.
