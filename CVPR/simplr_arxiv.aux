\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{he2022mae}
\citation{li2022vitdet}
\citation{nguyen2022boxer}
\citation{lin2023plaindetr}
\citation{he2022mae}
\citation{li2022vitdet}
\citation{nguyen2022boxer}
\citation{lin2023plaindetr}
\citation{vaswani2017transformer}
\citation{liu2021swintransformer,dosovitskiy2021vit}
\citation{nicolas2020detr,zhu2021deformable,nguyen2022boxer}
\citation{wang2021maxdeeplab,zhang2021knet,cheng2022mask2former}
\citation{brown2020gpt3,devlin2019bert}
\citation{liu2021swintransformer}
\citation{fan2021mvit,wang2021pvit,heo2021rethinkingvit}
\citation{tsung2017fpn}
\citation{li2022vitdet}
\citation{dosovitskiy2021vit}
\citation{he2022mae,bao2022beit,dehghani2023scalingvit22b}
\citation{nicolas2020detr}
\@LN@col{1}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.\nobreakspace  {}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:introduction}{{1}{1}{\hskip -1em.~Introduction}{section.1}{}}
\newlabel{sec:introduction@cref}{{[section][1][]1}{[1][1][]1}}
\@writefile{brf}{\backcite{vaswani2017transformer}{{1}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{dosovitskiy2021vit}{{1}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{liu2021swintransformer}{{1}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{nicolas2020detr}{{1}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{1}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{zhu2021deformable}{{1}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{cheng2022mask2former}{{1}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{wang2021maxdeeplab}{{1}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{zhang2021knet}{{1}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{brown2020gpt3}{{1}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{devlin2019bert}{{1}{1}{figure.caption.1}}}
\@LN@col{2}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  \textbf  {A plain detector is non-trivial.} Even with the use of a plain backbone ViT pre-trained using MAE\nobreakspace  {}\cite  {he2022mae}, feature pyramids are still important for both convolution-based (ViTDet\nobreakspace  {}\cite  {li2022vitdet}) and transformer-based (BoxeR\nobreakspace  {}\cite  {nguyen2022boxer}) detectors. While removing multi-scale input from the encoder, PlainDETR\nobreakspace  {}\cite  {lin2023plaindetr} still relies on feature pyramids for its box proposal generation and lags behind in performance. In this paper, we demonstrate that the plain detector, SimPLR\xspace  , with the proposed scale-aware attention yields competitive performance compared to multi-scale counterparts. \relax }}{1}{figure.caption.1}\protected@file@percent }
\@writefile{brf}{\backcite{he2022mae}{{1}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{li2022vitdet}{{1}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{1}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{lin2023plaindetr}{{1}{1}{figure.caption.1}}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:single_scale}{{1}{1}{\textbf {A plain detector is non-trivial.} Even with the use of a plain backbone ViT pre-trained using MAE~\cite {he2022mae}, feature pyramids are still important for both convolution-based (ViTDet~\cite {li2022vitdet}) and transformer-based (BoxeR~\cite {nguyen2022boxer}) detectors. While removing multi-scale input from the encoder, PlainDETR~\cite {lin2023plaindetr} still relies on feature pyramids for its box proposal generation and lags behind in performance. In this paper, we demonstrate that the plain detector, \ours , with the proposed scale-aware attention yields competitive performance compared to multi-scale counterparts. \relax }{figure.caption.1}{}}
\newlabel{fig:single_scale@cref}{{[figure][1][]1}{[1][1][]1}}
\@writefile{brf}{\backcite{liu2021swintransformer}{{1}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{fan2021mvit}{{1}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{heo2021rethinkingvit}{{1}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{wang2021pvit}{{1}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{tsung2017fpn}{{1}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{li2022vitdet}{{1}{1}{figure.caption.1}}}
\citation{dosovitskiy2021vit,li2022vitdet}
\citation{ren2015faster_rcnn,he2017maskrcnn,li2022vitdet}
\citation{zhu2021deformable,nguyen2022boxer,cheng2022mask2former}
\citation{nicolas2020detr,zhu2021deformable,nguyen2022boxer,cheng2022mask2former}
\citation{li2022vitdet}
\citation{cheng2022mask2former}
\citation{he2022mae}
\citation{peng2022beitv2}
\citation{girshick2014rcnn}
\citation{simonyan2015vgg,xie2017resnext,kaiming2016resnet,huang2017densenet}
\citation{lecun95convolutional}
\citation{deng2009imagenet}
\citation{brown2020gpt3,devlin2019bert}
\citation{chen2020igpt,dosovitskiy2021vit,liu2021swintransformer}
\citation{dosovitskiy2021vit}
\citation{chen2020simclr,he2022mae}
\citation{nicolas2020detr}
\citation{dong2021solq,wang2021maxdeeplab,nguyen2022boxer}
\citation{cheng2021maskformer}
\citation{zhang2021knet}
\citation{cheng2022mask2former}
\citation{yu2022kmmask}
\citation{zhang2023dino,li2023maskdino}
\citation{chen2022uvit}
\citation{li2022vitdet}
\citation{he2022mae}
\@LN@col{1}
\@writefile{brf}{\backcite{dosovitskiy2021vit}{{2}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{bao2022beit}{{2}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{dehghani2023scalingvit22b}{{2}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{he2022mae}{{2}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{nicolas2020detr}{{2}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{dosovitskiy2021vit}{{2}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{li2022vitdet}{{2}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{he2017maskrcnn}{{2}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{li2022vitdet}{{2}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{ren2015faster_rcnn}{{2}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{cheng2022mask2former}{{2}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{2}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{zhu2021deformable}{{2}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{nicolas2020detr}{{2}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{cheng2022mask2former}{{2}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{2}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{zhu2021deformable}{{2}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{li2022vitdet}{{2}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{cheng2022mask2former}{{2}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{he2022mae}{{2}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{peng2022beitv2}{{2}{1}{figure.caption.1}}}
\@LN@col{2}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.\nobreakspace  {}Related Work}{2}{section.2}\protected@file@percent }
\newlabel{sec:related_work}{{2}{2}{\hskip -1em.~Related Work}{section.2}{}}
\newlabel{sec:related_work@cref}{{[section][2][]2}{[1][2][]2}}
\@writefile{brf}{\backcite{girshick2014rcnn}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{kaiming2016resnet}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{huang2017densenet}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{simonyan2015vgg}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{xie2017resnext}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{lecun95convolutional}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{deng2009imagenet}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{brown2020gpt3}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{devlin2019bert}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{chen2020igpt}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{dosovitskiy2021vit}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{liu2021swintransformer}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{dosovitskiy2021vit}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{chen2020simclr}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{he2022mae}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{nicolas2020detr}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{dong2021solq}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{wang2021maxdeeplab}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{cheng2021maskformer}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{zhang2021knet}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{cheng2022mask2former}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{yu2022kmmask}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{li2023maskdino}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{zhang2023dino}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{chen2022uvit}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{li2022vitdet}{{2}{2}{section.2}}}
\citation{lin2023plaindetr}
\citation{jia2023hybridmatching}
\citation{zhu2021deformable,nguyen2022boxer}
\citation{zhu2021deformable,li2022vitdet,nguyen2022boxer}
\citation{nguyen2022boxer}
\citation{zhu2021deformable}
\citation{nguyen2022boxer}
\citation{zhu2021deformable}
\citation{li2022vitdet}
\citation{nguyen2022boxer,cheng2022mask2former}
\citation{liu2021swintransformer}
\citation{li2022vitdet}
\citation{nguyen2022boxer,cheng2022mask2former}
\citation{liu2021swintransformer}
\citation{wei2016ssd,tsung2017fpn,zhu2021deformable}
\citation{li2022vitdet,chen2022uvit}
\citation{vaswani2017transformer}
\@LN@col{1}
\@writefile{brf}{\backcite{he2022mae}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{lin2023plaindetr}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{jia2023hybridmatching}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{zhu2021deformable}{{3}{2}{section.2}}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.\nobreakspace  {}Background}{3}{section.3}\protected@file@percent }
\newlabel{sec:background}{{3}{3}{\hskip -1em.~Background}{section.3}{}}
\newlabel{sec:background@cref}{{[section][3][]3}{[1][3][]3}}
\@writefile{brf}{\backcite{li2022vitdet}{{3}{3}{section.3}}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{3}{3}{section.3}}}
\@writefile{brf}{\backcite{zhu2021deformable}{{3}{3}{section.3}}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{3}{3}{section.3}}}
\@writefile{brf}{\backcite{zhu2021deformable}{{3}{3}{section.3}}}
\@writefile{toc}{\contentsline {paragraph}{Multi-head Box-Attention.}{3}{section*.2}\protected@file@percent }
\@LN@col{2}
\@writefile{brf}{\backcite{nguyen2022boxer}{{3}{3}{equation.3.5}}}
\@writefile{toc}{\contentsline {paragraph}{Multi-head Deformable Attention.}{3}{section*.3}\protected@file@percent }
\@writefile{brf}{\backcite{zhu2021deformable}{{3}{3}{equation.3.8}}}
\citation{zhu2021deformable}
\citation{vaswani2017transformer}
\citation{nguyen2022boxer}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  \textbf  {Object detection architectures. Left:} The plain-backbone detector from\nobreakspace  {}\cite  {li2022vitdet} whose input (denoted in the dashed region) are multi-scale features. \textbf  {Middle:} State-of-the-art end-to-end detectors\nobreakspace  {}\cite  {nguyen2022boxer,cheng2022mask2former} utilize a hierarchical backbone (\emph  {i.e}\onedot  , Swin\nobreakspace  {}\cite  {liu2021swintransformer}) to create multi-scale inputs. \textbf  {Right:} Our simple single-scale detector following the end-to-end framework. Where existing detectors require feature pyramids to be effective, we propose a plain detector, SimPLR\xspace  , whose backbone and detection head are non-hierarchical and operate on a single-scale feature map. The plain detector, SimPLR\xspace  , achieves on par or even better performance compared to hierarchical and/or multi-scale counterparts while being more efficient. \relax }}{4}{figure.caption.4}\protected@file@percent }
\@writefile{brf}{\backcite{li2022vitdet}{{4}{2}{figure.caption.4}}}
\@writefile{brf}{\backcite{cheng2022mask2former}{{4}{2}{figure.caption.4}}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{4}{2}{figure.caption.4}}}
\@writefile{brf}{\backcite{liu2021swintransformer}{{4}{2}{figure.caption.4}}}
\newlabel{fig:compare}{{2}{4}{\textbf {Object detection architectures. Left:} The plain-backbone detector from~\cite {li2022vitdet} whose input (denoted in the dashed region) are multi-scale features. \textbf {Middle:} State-of-the-art end-to-end detectors~\cite {nguyen2022boxer,cheng2022mask2former} utilize a hierarchical backbone (\ie , Swin~\cite {liu2021swintransformer}) to create multi-scale inputs. \textbf {Right:} Our simple single-scale detector following the end-to-end framework. Where existing detectors require feature pyramids to be effective, we propose a plain detector, \ours , whose backbone and detection head are non-hierarchical and operate on a single-scale feature map. The plain detector, \ours , achieves on par or even better performance compared to hierarchical and/or multi-scale counterparts while being more efficient. \relax }{figure.caption.4}{}}
\newlabel{fig:compare@cref}{{[figure][2][]2}{[1][3][]4}}
\@LN@col{1}
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.\nobreakspace  {}SimPLR\xspace  : A Simple and Plain Detector}{4}{section.4}\protected@file@percent }
\newlabel{sec:simplr}{{4}{4}{\hskip -1em.~\ours : A Simple and Plain Detector}{section.4}{}}
\newlabel{sec:simplr@cref}{{[section][4][]4}{[1][3][]4}}
\@writefile{brf}{\backcite{tsung2017fpn}{{4}{4}{section.4}}}
\@writefile{brf}{\backcite{wei2016ssd}{{4}{4}{section.4}}}
\@writefile{brf}{\backcite{zhu2021deformable}{{4}{4}{section.4}}}
\@writefile{brf}{\backcite{chen2022uvit}{{4}{4}{section.4}}}
\@writefile{brf}{\backcite{li2022vitdet}{{4}{4}{section.4}}}
\@writefile{brf}{\backcite{vaswani2017transformer}{{4}{4}{section.4}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}\hskip -1em.\nobreakspace  {}Scale-aware attention}{4}{subsection.4.1}\protected@file@percent }
\@LN@col{2}
\@writefile{brf}{\backcite{zhu2021deformable}{{4}{4.1}{subsection.4.1}}}
\@writefile{brf}{\backcite{vaswani2017transformer}{{4}{4.1}{subsection.4.1}}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{4}{4.1}{subsection.4.1}}}
\citation{nguyen2022boxer}
\citation{li2022vitdet}
\citation{li2022vitdet}
\citation{he2022mae}
\citation{peng2022beitv2}
\citation{he2022mae,zhai2022scalingvit,dehghani2023scalingvit22b}
\citation{kirillov2019panoptic}
\citation{cheng2022mask2former}
\citation{cheng2022mask2former}
\citation{nguyen2022boxer}
\citation{cheng2022mask2former}
\@LN@col{1}
\@LN@col{2}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}\hskip -1em.\nobreakspace  {}Network Architecture}{5}{subsection.4.2}\protected@file@percent }
\@writefile{brf}{\backcite{nguyen2022boxer}{{5}{4.2}{subsection.4.2}}}
\@writefile{brf}{\backcite{li2022vitdet}{{5}{4.2}{subsection.4.2}}}
\@writefile{brf}{\backcite{li2022vitdet}{{5}{4.2}{subsection.4.2}}}
\@writefile{brf}{\backcite{he2022mae}{{5}{4.2}{subsection.4.2}}}
\@writefile{brf}{\backcite{peng2022beitv2}{{5}{4.2}{subsection.4.2}}}
\@writefile{brf}{\backcite{dehghani2023scalingvit22b}{{5}{4.2}{subsection.4.2}}}
\@writefile{brf}{\backcite{he2022mae}{{5}{4.2}{subsection.4.2}}}
\@writefile{brf}{\backcite{zhai2022scalingvit}{{5}{4.2}{subsection.4.2}}}
\@writefile{brf}{\backcite{kirillov2019panoptic}{{5}{4.2}{subsection.4.2}}}
\@writefile{brf}{\backcite{cheng2022mask2former}{{5}{4.2}{subsection.4.2}}}
\@writefile{brf}{\backcite{cheng2022mask2former}{{5}{4.2}{subsection.4.2}}}
\citation{lin2014mscoco}
\citation{he2022mae}
\citation{nguyen2022boxer}
\citation{loshchilov2019adamw}
\citation{dosovitskiy2021vit}
\citation{shiasi2021lsjitter}
\citation{nguyen2022boxer}
\citation{li2022vitdet}
\citation{zhu2021deformable}
\citation{lin2023plaindetr}
\citation{lin2023plaindetr}
\citation{lin2023plaindetr}
\citation{li2022vitdet}
\citation{li2022vitdet}
\citation{nguyen2022boxer}
\citation{nguyen2022boxer}
\citation{zhu2021deformable}
\citation{li2022vitdet}
\citation{lin2023plaindetr}
\@LN@col{1}
\@writefile{brf}{\backcite{nguyen2022boxer}{{6}{4.2}{subsection.4.2}}}
\@writefile{brf}{\backcite{cheng2022mask2former}{{6}{4.2}{subsection.4.2}}}
\@writefile{toc}{\contentsline {section}{\numberline {5}\hskip -1em.\nobreakspace  {}Experiments}{6}{section.5}\protected@file@percent }
\newlabel{sec:experiments}{{5}{6}{\hskip -1em.~Experiments}{section.5}{}}
\newlabel{sec:experiments@cref}{{[section][5][]5}{[1][6][]6}}
\@writefile{brf}{\backcite{lin2014mscoco}{{6}{5}{section.5}}}
\@writefile{brf}{\backcite{he2022mae}{{6}{5}{section.5}}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{6}{5}{section.5}}}
\@writefile{brf}{\backcite{loshchilov2019adamw}{{6}{5}{section.5}}}
\@writefile{brf}{\backcite{dosovitskiy2021vit}{{6}{5}{section.5}}}
\@writefile{brf}{\backcite{shiasi2021lsjitter}{{6}{5}{section.5}}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{6}{5}{section.5}}}
\@writefile{brf}{\backcite{li2022vitdet}{{6}{5}{section.5}}}
\@LN@col{2}
\@writefile{brf}{\backcite{lin2023plaindetr}{{6}{\caption@xref {??}{ on input line 25}}{table.caption.6}}}
\@writefile{brf}{\backcite{lin2023plaindetr}{{6}{\caption@xref {??}{ on input line 26}}{table.caption.6}}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \textbf  {SimPLR\xspace  is an effective plain detector.} All methods use ViT-B as backbone. Methods that take feature pyramids as input employ SimpleFPN with ViT from \cite  {li2022vitdet}. Our plain detector, SimPLR\xspace  , shows competitive performance compared to multi-scale alternatives, while being more efficient in terms of FLOPs, training memory and faster during inference. Training memory is reported as relative to SimPLR\xspace  (SAD: scale-aware deformable attention; SAB: scale-aware box-attention).\relax }}{6}{table.caption.6}\protected@file@percent }
\@writefile{brf}{\backcite{li2022vitdet}{{6}{1}{table.caption.6}}}
\newlabel{tab:compare}{{1}{6}{\textbf {\ours is an effective plain detector.} All methods use ViT-B as backbone. Methods that take feature pyramids as input employ SimpleFPN with ViT from \cite {li2022vitdet}. Our plain detector, \ours , shows competitive performance compared to multi-scale alternatives, while being more efficient in terms of FLOPs, training memory and faster during inference. Training memory is reported as relative to \ours (SAD: scale-aware deformable attention; SAB: scale-aware box-attention).\relax }{table.caption.6}{}}
\newlabel{tab:compare@cref}{{[table][1][]1}{[1][6][]6}}
\@writefile{brf}{\backcite{zhu2021deformable}{{6}{5}{section.5}}}
\@writefile{brf}{\backcite{lin2023plaindetr}{{6}{5}{section.5}}}
\@writefile{brf}{\backcite{zhu2021deformable}{{6}{5}{table.caption.7}}}
\@writefile{brf}{\backcite{li2022vitdet}{{6}{5}{table.caption.7}}}
\citation{nguyen2022boxer}
\citation{he2022mae}
\citation{peng2022beitv2}
\citation{cheng2022mask2former}
\citation{liu2021swintransformer}
\citation{fan2021mvit}
\citation{liu2021swintransformer}
\citation{cheng2022mask2former}
\citation{fan2021mvit}
\citation{nguyen2022boxer}
\citation{li2022vitdet}
\citation{chen2022uvit}
\citation{lin2023plaindetr}
\citation{liu2021swintransformer}
\citation{cheng2022mask2former}
\citation{fan2021mvit}
\citation{li2022vitdet}
\citation{li2022vitdet}
\citation{li2022vitdet}
\citation{cheng2021maskformer}
\citation{cheng2022mask2former}
\citation{cheng2022mask2former}
\newlabel{tab:strat}{{2a}{7}{\textbf {Scale-aware attention.}\caption@thelabel \relax }{table.caption.7}{}}
\newlabel{tab:strat@cref}{{[subtable][1][2]2a}{[1][6][]7}}
\newlabel{sub@tab:strat}{{a}{7}{\textbf {Scale-aware attention.}\caption@thelabel \relax }{table.caption.7}{}}
\newlabel{sub@tab:strat@cref}{{[subtable][1][2]2a}{[1][6][]7}}
\newlabel{tab:window_size}{{2b}{7}{\textbf {Window size.}\caption@thelabel \relax }{table.caption.7}{}}
\newlabel{tab:window_size@cref}{{[subtable][2][2]2b}{[1][6][]7}}
\newlabel{sub@tab:window_size}{{b}{7}{\textbf {Window size.}\caption@thelabel \relax }{table.caption.7}{}}
\newlabel{sub@tab:window_size@cref}{{[subtable][2][2]2b}{[1][6][]7}}
\newlabel{tab:num_scale}{{2c}{7}{\textbf {Number of window scales}.\caption@thelabel \relax }{table.caption.7}{}}
\newlabel{tab:num_scale@cref}{{[subtable][3][2]2c}{[1][6][]7}}
\newlabel{sub@tab:num_scale}{{c}{7}{\textbf {Number of window scales}.\caption@thelabel \relax }{table.caption.7}{}}
\newlabel{sub@tab:num_scale@cref}{{[subtable][3][2]2c}{[1][6][]7}}
\newlabel{tab:feat_scale}{{2d}{7}{\textbf {Scales of input features.}\caption@thelabel \relax }{table.caption.7}{}}
\newlabel{tab:feat_scale@cref}{{[subtable][4][2]2d}{[1][6][]7}}
\newlabel{sub@tab:feat_scale}{{d}{7}{\textbf {Scales of input features.}\caption@thelabel \relax }{table.caption.7}{}}
\newlabel{sub@tab:feat_scale@cref}{{[subtable][4][2]2d}{[1][6][]7}}
\newlabel{fig:attn_vis}{{2e}{7}{Visualization of scale distribution learnt in \textbf {multi-head adaptive-scale attention} of object proposals. Objects are classified into \emph {small}, \emph {medium}, and \emph {large} based on their area.\caption@thelabel \relax }{table.caption.7}{}}
\newlabel{fig:attn_vis@cref}{{[subtable][5][2]2e}{[1][6][]7}}
\newlabel{sub@fig:attn_vis}{{e}{7}{Visualization of scale distribution learnt in \textbf {multi-head adaptive-scale attention} of object proposals. Objects are classified into \emph {small}, \emph {medium}, and \emph {large} based on their area.\caption@thelabel \relax }{table.caption.7}{}}
\newlabel{sub@fig:attn_vis@cref}{{[subtable][5][2]2e}{[1][6][]7}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces  \textbf  {Ablation of scale-aware attention} in SimPLR\xspace  using a plain ViT backbone on COCO\nobreakspace  {}{\texttt  {val}}\xspace  . \textbf  {Table (a-d):} Compared to the na\"ive baseline, which employs BoxeR and box-attention \citep  {nguyen2022boxer} with \textit  {single-scale} features, our plain detector, SimPLR\xspace  , with scale-aware attention improves the performance consistently for all settings, default setting highlighted. \textbf  {Figure e:} our adaptive-scale attention captures different scale distribution in its attention heads based on the context of query vectors. Specifically, queries of \emph  {small} objects tends to focus on reference windows of small scales (\emph  {i.e}\onedot  , mainly \(32\times 32\)), while query vectors of \emph  {medium} and \emph  {large} objects distribute more attention computation into larger reference windows.\relax }}{7}{table.caption.7}\protected@file@percent }
\newlabel{tab:det_ablation}{{2}{7}{\textbf {Ablation of scale-aware attention} in \ours using a plain ViT backbone on COCO~\val . \textbf {Table (a-d):} Compared to the na\"ive baseline, which employs BoxeR and box-attention \citep {nguyen2022boxer} with \textit {single-scale} features, our plain detector, \ours , with scale-aware attention improves the performance consistently for all settings, default setting highlighted. \textbf {Figure e:} our adaptive-scale attention captures different scale distribution in its attention heads based on the context of query vectors. Specifically, queries of \emph {small} objects tends to focus on reference windows of small scales (\ie , mainly \(32\times 32\)), while query vectors of \emph {medium} and \emph {large} objects distribute more attention computation into larger reference windows.\relax }{table.caption.7}{}}
\newlabel{tab:det_ablation@cref}{{[table][2][]2}{[1][6][]7}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{7}{2}{table.caption.7}}}
\@LN@col{1}
\@writefile{brf}{\backcite{lin2023plaindetr}{{7}{5}{table.caption.7}}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{7}{5}{table.caption.7}}}
\@LN@col{2}
\@writefile{brf}{\backcite{he2022mae}{{7}{5}{table.caption.7}}}
\@writefile{brf}{\backcite{peng2022beitv2}{{7}{5}{table.caption.7}}}
\@writefile{brf}{\backcite{cheng2022mask2former}{{7}{5}{table.caption.7}}}
\@writefile{brf}{\backcite{liu2021swintransformer}{{7}{5}{table.caption.7}}}
\@writefile{brf}{\backcite{fan2021mvit}{{7}{5}{table.caption.7}}}
\@writefile{brf}{\backcite{liu2021swintransformer}{{8}{\caption@xref {??}{ on input line 174}}{table.caption.8}}}
\@writefile{brf}{\backcite{cheng2022mask2former}{{8}{\caption@xref {??}{ on input line 175}}{table.caption.8}}}
\@writefile{brf}{\backcite{fan2021mvit}{{8}{\caption@xref {??}{ on input line 176}}{table.caption.8}}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{8}{\caption@xref {??}{ on input line 177}}{table.caption.8}}}
\@writefile{brf}{\backcite{li2022vitdet}{{8}{\caption@xref {??}{ on input line 178}}{table.caption.8}}}
\@writefile{brf}{\backcite{chen2022uvit}{{8}{\caption@xref {??}{ on input line 181}}{table.caption.8}}}
\@writefile{brf}{\backcite{lin2023plaindetr}{{8}{\caption@xref {??}{ on input line 182}}{table.caption.8}}}
\@writefile{brf}{\backcite{liu2021swintransformer}{{8}{\caption@xref {??}{ on input line 188}}{table.caption.8}}}
\@writefile{brf}{\backcite{cheng2022mask2former}{{8}{\caption@xref {??}{ on input line 189}}{table.caption.8}}}
\@writefile{brf}{\backcite{fan2021mvit}{{8}{\caption@xref {??}{ on input line 190}}{table.caption.8}}}
\@writefile{brf}{\backcite{li2022vitdet}{{8}{\caption@xref {??}{ on input line 191}}{table.caption.8}}}
\@writefile{brf}{\backcite{li2022vitdet}{{8}{\caption@xref {??}{ on input line 199}}{table.caption.8}}}
\@writefile{brf}{\backcite{li2022vitdet}{{8}{\caption@xref {??}{ on input line 200}}{table.caption.8}}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces \textbf  { State-of-the-art comparison and scaling behavior for object detection and instance segmentation.} We compare methods using feature pyramids (\emph  {top} row) \emph  {vs}\onedot  single-scale (\emph  {bottom} row) on COCO {\texttt  {val}}\xspace  . Backbones with MAE pre-trained on ImageNet-1K while others pre-trained on ImageNet-22K. Methods in \textcolor {gray}{gray color} are with convolution-based detection head. (n/a: entry is not available). Models of larger sizes are grouped with \emph  {darker} \textcolor {orange}{orange color}. SimPLR\xspace  indicates good scaling behavior. With only single-scale features, SimPLR\xspace  shows strong performance compared to multi-scale detectors including transformer-based detectors like Mask2Former, while being faster.\relax }}{8}{table.caption.8}\protected@file@percent }
\newlabel{tab:det_main}{{3}{8}{\textbf { State-of-the-art comparison and scaling behavior for object detection and instance segmentation.} We compare methods using feature pyramids (\emph {top} row) \vs single-scale (\emph {bottom} row) on COCO \val . Backbones with MAE pre-trained on ImageNet-1K while others pre-trained on ImageNet-22K. Methods in \textcolor {gray}{gray color} are with convolution-based detection head. (n/a: entry is not available). Models of larger sizes are grouped with \emph {darker} \textcolor {orange}{orange color}. \ours indicates good scaling behavior. With only single-scale features, \ours shows strong performance compared to multi-scale detectors including transformer-based detectors like Mask2Former, while being faster.\relax }{table.caption.8}{}}
\newlabel{tab:det_main@cref}{{[table][3][]3}{[1][7][]8}}
\@LN@col{1}
\@writefile{brf}{\backcite{cheng2021maskformer}{{8}{\caption@xref {??}{ on input line 223}}{table.caption.9}}}
\@writefile{brf}{\backcite{cheng2022mask2former}{{8}{\caption@xref {??}{ on input line 224}}{table.caption.9}}}
\@writefile{brf}{\backcite{cheng2022mask2former}{{8}{\caption@xref {??}{ on input line 229}}{table.caption.9}}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces \textbf  {State-of-the-art comparison and scaling behavior for panoptic segmentation.} We compare between methods using feature pyramids (\emph  {top} row) \emph  {vs}\onedot  single-scale (\emph  {bottom} row) on COCO\nobreakspace  {}{\texttt  {val}}\xspace  . Models of larger sizes are with \emph  {darker} \textcolor {orange}{orange color}. SimPLR\xspace  shows better results when scaling to larger backbones, while being faster with single-scale input.\relax }}{8}{table.caption.9}\protected@file@percent }
\newlabel{tab:panoptic}{{4}{8}{\textbf {State-of-the-art comparison and scaling behavior for panoptic segmentation.} We compare between methods using feature pyramids (\emph {top} row) \vs single-scale (\emph {bottom} row) on COCO~\val . Models of larger sizes are with \emph {darker} \textcolor {orange}{orange color}. \ours shows better results when scaling to larger backbones, while being faster with single-scale input.\relax }{table.caption.9}{}}
\newlabel{tab:panoptic@cref}{{[table][4][]4}{[1][7][]8}}
\@LN@col{2}
\@writefile{toc}{\contentsline {section}{\numberline {6}\hskip -1em.\nobreakspace  {}Conclusion}{8}{section.6}\protected@file@percent }
\newlabel{sec:conclusion}{{6}{8}{\hskip -1em.~Conclusion}{section.6}{}}
\newlabel{sec:conclusion@cref}{{[section][6][]6}{[1][8][]8}}
\citation{nguyen2022boxer}
\citation{nguyen2022boxer}
\citation{nguyen2022boxer}
\citation{he2017maskrcnn}
\citation{cheng2022mask2former}
\citation{nguyen2022boxer}
\citation{li2022vitdet}
\citation{wu2018groupnorm}
\citation{li2022vitdet}
\citation{li2022vitdet}
\citation{lin2017focalloss}
\citation{milletari2016vnet}
\citation{rezatofighi2019giou}
\citation{nguyen2022boxer}
\citation{li2022vitdet}
\@LN@col{1}
\@writefile{toc}{\contentsline {section}{\numberline {A}\hskip -1em.\nobreakspace  {}Implementation Details}{9}{appendix.A}\protected@file@percent }
\@writefile{brf}{\backcite{nguyen2022boxer}{{9}{A}{figure.caption.11}}}
\@writefile{brf}{\backcite{he2017maskrcnn}{{9}{A}{figure.caption.11}}}
\@writefile{brf}{\backcite{cheng2022mask2former}{{9}{A}{equation.A.11}}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Masked Instance-Attention. Left:} The box-attention\nobreakspace  {}\citep  {nguyen2022boxer} which samples $2\times 2$ grid features in the region of interest. \textbf  {Right:} Our masked instance-attention for dense grid sampling that employs masking strategy to capture object boundary. The $2\times 2$ attention scores are denoted in four colours and the masked attention score is shown in white.\relax }}{9}{figure.caption.11}\protected@file@percent }
\@writefile{brf}{\backcite{nguyen2022boxer}{{9}{3}{figure.caption.11}}}
\newlabel{fig:masked_instance_attn}{{3}{9}{\textbf {Masked Instance-Attention. Left:} The box-attention~\citep {nguyen2022boxer} which samples $2\times 2$ grid features in the region of interest. \textbf {Right:} Our masked instance-attention for dense grid sampling that employs masking strategy to capture object boundary. The $2\times 2$ attention scores are denoted in four colours and the masked attention score is shown in white.\relax }{figure.caption.11}{}}
\newlabel{fig:masked_instance_attn@cref}{{[figure][3][2147483647]3}{[1][9][]9}}
\@LN@col{2}
\@writefile{brf}{\backcite{nguyen2022boxer}{{9}{A}{equation.A.11}}}
\@writefile{brf}{\backcite{li2022vitdet}{{9}{A}{equation.A.11}}}
\@writefile{brf}{\backcite{wu2018groupnorm}{{9}{A}{equation.A.11}}}
\@writefile{brf}{\backcite{lin2017focalloss}{{9}{A}{figure.caption.12}}}
\@writefile{brf}{\backcite{milletari2016vnet}{{9}{A}{figure.caption.12}}}
\@writefile{brf}{\backcite{rezatofighi2019giou}{{9}{A}{figure.caption.12}}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{9}{A}{figure.caption.12}}}
\@writefile{brf}{\backcite{li2022vitdet}{{9}{A}{figure.caption.12}}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Hyper-parameters of backbone and detection head for different sizes of SimPLR\xspace  (base -- large -- huge models). Note that these settings are the same for all three tasks.\relax }}{9}{table.caption.13}\protected@file@percent }
\newlabel{tab:hyper}{{5}{9}{Hyper-parameters of backbone and detection head for different sizes of \ours (base -- large -- huge models). Note that these settings are the same for all three tasks.\relax }{table.caption.13}{}}
\newlabel{tab:hyper@cref}{{[table][5][2147483647]5}{[1][9][]9}}
\citation{nguyen2022boxer}
\citation{nguyen2022boxer}
\citation{touvron2022deit3}
\citation{touvron2021deit}
\citation{he2022mae}
\citation{li2022vitdet}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  \textbf  {The creation of input features. Left:} The creation of feature pyramids from the last feature of the plain backbone, ViT, in SimpleFPN\nobreakspace  {}\citep  {li2022vitdet} where different stacks of convolutional layers are used to create features at different scales. \textbf  {Right:} The design of our single-scale feature map with only one layer. \relax }}{10}{figure.caption.12}\protected@file@percent }
\@writefile{brf}{\backcite{li2022vitdet}{{10}{4}{figure.caption.12}}}
\newlabel{fig:fpn_vs_ss}{{4}{10}{\textbf {The creation of input features. Left:} The creation of feature pyramids from the last feature of the plain backbone, ViT, in SimpleFPN~\citep {li2022vitdet} where different stacks of convolutional layers are used to create features at different scales. \textbf {Right:} The design of our single-scale feature map with only one layer. \relax }{figure.caption.12}{}}
\newlabel{fig:fpn_vs_ss@cref}{{[figure][4][2147483647]4}{[1][9][]10}}
\@LN@col{1}
\@writefile{toc}{\contentsline {section}{\numberline {B}\hskip -1em.\nobreakspace  {}Additional Results}{10}{appendix.B}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces \textbf  {More panoptic segmentation comparison} between SimPLR\xspace  with ViT-B backbone and other methods with Swin-B backbone. All backbones are pre-trained on ImageNet-1K with supervised pre-training. SimPLR\xspace  still shows competitive results when using only single-scale input.\relax }}{10}{table.caption.14}\protected@file@percent }
\newlabel{tab:more_panop}{{6}{10}{\textbf {More panoptic segmentation comparison} between \ours with ViT-B backbone and other methods with Swin-B backbone. All backbones are pre-trained on ImageNet-1K with supervised pre-training. \ours still shows competitive results when using only single-scale input.\relax }{table.caption.14}{}}
\newlabel{tab:more_panop@cref}{{[table][6][2147483647]6}{[1][9][]10}}
\@writefile{brf}{\backcite{touvron2022deit3}{{10}{B}{table.caption.15}}}
\@writefile{brf}{\backcite{touvron2021deit}{{10}{B}{table.caption.15}}}
\@writefile{brf}{\backcite{he2022mae}{{10}{B}{table.caption.15}}}
\@writefile{brf}{\backcite{li2022vitdet}{{10}{B}{table.caption.15}}}
\@LN@col{2}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {Qualitative results} generated by SimPLR\xspace  using ViT-B as backbone on the COCO {\texttt  {val}}\xspace  set. In each pair, the left image shows the visualization of instance detection and segmentation, while the right one indicates the panoptic segmentation.\relax }}{10}{figure.caption.16}\protected@file@percent }
\newlabel{fig:vis_sup}{{5}{10}{\textbf {Qualitative results} generated by \ours using ViT-B as backbone on the COCO \val set. In each pair, the left image shows the visualization of instance detection and segmentation, while the right one indicates the panoptic segmentation.\relax }{figure.caption.16}{}}
\newlabel{fig:vis_sup@cref}{{[figure][5][2147483647]5}{[1][10][]10}}
\citation{deng2009imagenet}
\citation{lin2014mscoco}
\bibstyle{ieeenat_fullname}
\bibdata{simplr}
\bibcite{bao2022beit}{{1}{2022}{{Bao et~al.}}{{Bao, Dong, Piao, and Wei}}}
\bibcite{brown2020gpt3}{{2}{2020}{{Brown et~al.}}{{Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Amanda~Askell, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei}}}
\bibcite{nicolas2020detr}{{3}{2020}{{Carion et~al.}}{{Carion, Massa, Synnaeve, Usunier, Kirillov, and Zagoruyko}}}
\bibcite{chen2020igpt}{{4}{2020{}}{{Chen et~al.}}{{Chen, Radford, Child, Wu, Jun, Dhariwal, Luan, and Sutskever}}}
\bibcite{chen2020simclr}{{5}{2020{}}{{Chen et~al.}}{{Chen, Kornblith, Norouzi, and Hinton}}}
\bibcite{chen2022uvit}{{6}{2022}{{Chen et~al.}}{{Chen, Du, Yang, Beyer, Zhai, Lin, Chen, Li, Song, Wang, and Zhou}}}
\bibcite{cheng2021maskformer}{{7}{2021}{{Cheng et~al.}}{{Cheng, Schwing, and Kirillov}}}
\bibcite{cheng2022mask2former}{{8}{2022}{{Cheng et~al.}}{{Cheng, Misra, Schwing, Kirillov, and Girdhar}}}
\bibcite{dehghani2023scalingvit22b}{{9}{2023}{{Dehghani et~al.}}{{Dehghani, Djolonga, Mustafa, and et~al.}}}
\bibcite{deng2009imagenet}{{10}{2009}{{Deng et~al.}}{{Deng, Dong, Socher, Li, Li, and Fei-Fei}}}
\bibcite{devlin2019bert}{{11}{2019}{{Devlin et~al.}}{{Devlin, Chang, Lee, and Toutanova}}}
\bibcite{dong2021solq}{{12}{2021}{{Dong et~al.}}{{Dong, Zeng, Wang, Zhang, and Wei}}}
\bibcite{dosovitskiy2021vit}{{13}{2021}{{Dosovitskiy et~al.}}{{Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby}}}
\bibcite{fan2021mvit}{{14}{2021}{{Fan et~al.}}{{Fan, Xiong, Mangalam, Li, Yan, Malik, and Feichtenhofer}}}
\bibcite{shiasi2021lsjitter}{{15}{2021}{{Ghiasi et~al.}}{{Ghiasi, Cui, Srinivas, Qian, Lin, Cubuk, Le, and Zoph}}}
\bibcite{girshick2014rcnn}{{16}{2014}{{Girshick et~al.}}{{Girshick, Donahue, Darrell, and Malik}}}
\bibcite{kaiming2016resnet}{{17}{2016}{{He et~al.}}{{He, Zhang, Ren, and Sun}}}
\bibcite{he2017maskrcnn}{{18}{2017}{{He et~al.}}{{He, Gkioxari, Dollár, and Girshick}}}
\bibcite{he2022mae}{{19}{2022}{{He et~al.}}{{He, Chen, Xie, Li, Doll{\'a}r, and Girshick}}}
\bibcite{heo2021rethinkingvit}{{20}{2021}{{Heo et~al.}}{{Heo, Yun, Han, Chun, Choe, and Oh}}}
\bibcite{huang2017densenet}{{21}{2017}{{Huang et~al.}}{{Huang, Liu, van~der Maaten, and Weinberger}}}
\bibcite{jia2023hybridmatching}{{22}{2023}{{Jia et~al.}}{{Jia, Yuan, He, Wu, Yu, Lin, Sun, Zhang, and Hu}}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces  \textbf  {Ablation on pre-training strategies} of the plain ViT backbone using SimPLR\xspace  evaluated on COCO object detection and instance segmentation. We compare the ViT backbone pre-trained using supervised methods (\emph  {top} row) \emph  {vs}\onedot  self-supervised methods (\emph  {bottom} row) with different sizes of pre-training dataset (ImageNet-1K \emph  {vs}\onedot  ImageNet-21K). Here, we use the $5\times $ schedule as in \cite  {nguyen2022boxer}. It can be seen that SimPLR\xspace  with the plain ViT backbone benefits from better pre-training approaches and with more pre-training data. \relax }}{11}{table.caption.15}\protected@file@percent }
\@writefile{brf}{\backcite{nguyen2022boxer}{{11}{7}{table.caption.15}}}
\newlabel{tab:pretrain}{{7}{11}{\textbf {Ablation on pre-training strategies} of the plain ViT backbone using \ours evaluated on COCO object detection and instance segmentation. We compare the ViT backbone pre-trained using supervised methods (\emph {top} row) \vs self-supervised methods (\emph {bottom} row) with different sizes of pre-training dataset (ImageNet-1K \vs ImageNet-21K). Here, we use the $5\times $ schedule as in \cite {nguyen2022boxer}. It can be seen that \ours with the plain ViT backbone benefits from better pre-training approaches and with more pre-training data. \relax }{table.caption.15}{}}
\newlabel{tab:pretrain@cref}{{[table][7][2147483647]7}{[1][10][]11}}
\@LN@col{1}
\@writefile{toc}{\contentsline {section}{\numberline {C}\hskip -1em.\nobreakspace  {}Qualitative results}{11}{appendix.C}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {D}\hskip -1em.\nobreakspace  {}Asset Licenses}{11}{appendix.D}\protected@file@percent }
\@writefile{brf}{\backcite{deng2009imagenet}{{11}{D}{appendix.D}}}
\@writefile{brf}{\backcite{lin2014mscoco}{{11}{D}{appendix.D}}}
\@LN@col{2}
\bibcite{kirillov2019panoptic}{{23}{2019}{{Kirillov et~al.}}{{Kirillov, He, Girshick, Rother, and Dollar}}}
\bibcite{lecun95convolutional}{{24}{1995}{{LeCun and Bengio}}{{}}}
\bibcite{li2023maskdino}{{25}{2023}{{Li et~al.}}{{Li, Zhang, xu, Liu, Zhang, Ni, and Shum}}}
\bibcite{li2022vitdet}{{26}{2022}{{Li et~al.}}{{Li, Mao, Girshick, and He}}}
\bibcite{lin2014mscoco}{{27}{2014}{{Lin et~al.}}{{Lin, Maire, Belongie, Bourdev, Girshick, Hays, Perona, Ramanan, Doll{\'{a}}r, and Zitnick}}}
\bibcite{lin2017focalloss}{{28}{2017}{{Lin et~al.}}{{Lin, Goyal, Girshick, He, and Doll{\'{a}}r}}}
\bibcite{tsung2017fpn}{{29}{2017}{{{Lin} et~al.}}{{{Lin}, {Dollár}, {Girshick}, {He}, {Hariharan}, and {Belongie}}}}
\bibcite{lin2023plaindetr}{{30}{2023}{{Lin et~al.}}{{Lin, Yuan, Zhang, Li, Zheng, and Hu}}}
\bibcite{wei2016ssd}{{31}{2016}{{Liu et~al.}}{{Liu, Anguelov, Erhan, Szegedy, Reed, Fu, and Berg}}}
\bibcite{liu2021swintransformer}{{32}{2021}{{Liu et~al.}}{{Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and Guo}}}
\bibcite{loshchilov2019adamw}{{33}{2019}{{Loshchilov and Hutter}}{{}}}
\bibcite{milletari2016vnet}{{34}{2016}{{Milletari et~al.}}{{Milletari, Navab, and Ahmadi}}}
\bibcite{nguyen2022boxer}{{35}{2022}{{Nguyen et~al.}}{{Nguyen, Ju, Booij, Oswald, and Snoek}}}
\bibcite{peng2022beitv2}{{36}{2022}{{Peng et~al.}}{{Peng, Dong, Bao, Ye, and Wei}}}
\bibcite{ren2015faster_rcnn}{{37}{2015}{{Ren et~al.}}{{Ren, He, Girshick, and Sun}}}
\bibcite{rezatofighi2019giou}{{38}{2019}{{Rezatofighi et~al.}}{{Rezatofighi, Tsoi, Gwak, Sadeghian, Reid, and Savarese}}}
\bibcite{simonyan2015vgg}{{39}{2015}{{Simonyan and Zisserman}}{{}}}
\bibcite{touvron2021deit}{{40}{2021}{{Touvron et~al.}}{{Touvron, Cord, Douze, Massa, Sablayrolles, and Jegou}}}
\bibcite{touvron2022deit3}{{41}{2022}{{Touvron et~al.}}{{Touvron, Cord, and Jegou}}}
\bibcite{vaswani2017transformer}{{42}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{wang2021maxdeeplab}{{43}{2021{}}{{Wang et~al.}}{{Wang, Zhu, Adam, Yuille, and Chen}}}
\bibcite{wang2021pvit}{{44}{2021{}}{{Wang et~al.}}{{Wang, Xie, Li, Fan, Song, Liang, Lu, Luo, and Shao}}}
\bibcite{wu2018groupnorm}{{45}{2018}{{Wu and He}}{{}}}
\bibcite{xie2017resnext}{{46}{2017}{{Xie et~al.}}{{Xie, Girshick, Doll{\'{a}}r, Tu, and He}}}
\bibcite{yu2022kmmask}{{47}{2022}{{Yu et~al.}}{{Yu, Wang, Qiao, Collins, Zhu, Adam, Yuille, and Chen}}}
\bibcite{zhai2022scalingvit}{{48}{2022}{{Zhai et~al.}}{{Zhai, Kolesnikov, Houlsby, and Beyer}}}
\bibcite{zhang2023dino}{{49}{2023}{{Zhang et~al.}}{{Zhang, Li, Liu, Zhang, Su, Zhu, Ni, and Shum}}}
\bibcite{zhang2021knet}{{50}{2021}{{Zhang et~al.}}{{Zhang, Pang, Chen, and Loy}}}
\bibcite{zhu2021deformable}{{51}{2021}{{Zhu et~al.}}{{Zhu, Su, Lu, Li, Wang, and Dai}}}
\@LN@col{1}
\@LN@col{2}
\gdef \@abspage@last{12}
