\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{nguyen2022boxer}
\citation{nguyen2022boxer}
\citation{nguyen2022boxer}
\citation{he2017maskrcnn}
\citation{cheng2022mask2former}
\citation{nguyen2022boxer}
\citation{li2022vitdet}
\citation{wu2018groupnorm}
\citation{li2022vitdet}
\citation{li2022vitdet}
\citation{lin2017focalloss}
\citation{milletari2016vnet}
\citation{rezatofighi2019giou}
\@LN@col{1}
\@LN{0}{0}
\@LN{1}{0}
\@LN{2}{0}
\@LN{3}{0}
\@LN{4}{0}
\@LN{5}{0}
\@LN{6}{0}
\@LN{7}{0}
\@LN{8}{0}
\@LN{9}{0}
\@writefile{toc}{\contentsline {section}{\numberline {A}\hskip -1em.\nobreakspace  {}Implementation Details}{1}{appendix.A}\protected@file@percent }
\@LN{10}{0}
\@LN{11}{0}
\@writefile{brf}{\backcite{nguyen2022boxer}{{1}{A}{figure.caption.2}}}
\@LN{12}{0}
\@LN{13}{0}
\@LN{14}{0}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Masked Instance-Attention. Left:} The box-attention\nobreakspace  {}\citep  {nguyen2022boxer} which samples $2\times 2$ grid features in the region of interest. \textbf  {Right:} Our masked instance-attention for dense grid sampling that employs masking strategy to capture object boundary. The $2\times 2$ attention scores are denoted in four colours and the masked attention score is shown in white.\relax }}{1}{figure.caption.2}\protected@file@percent }
\@writefile{brf}{\backcite{nguyen2022boxer}{{1}{1}{figure.caption.2}}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:masked_instance_attn}{{1}{1}{\textbf {Masked Instance-Attention. Left:} The box-attention~\citep {nguyen2022boxer} which samples $2\times 2$ grid features in the region of interest. \textbf {Right:} Our masked instance-attention for dense grid sampling that employs masking strategy to capture object boundary. The $2\times 2$ attention scores are denoted in four colours and the masked attention score is shown in white.\relax }{figure.caption.2}{}}
\newlabel{fig:masked_instance_attn@cref}{{[figure][1][2147483647]1}{[1][1][]1}}
\@LN@col{2}
\@LN{15}{0}
\@LN{16}{0}
\@LN{17}{0}
\@LN{18}{0}
\@LN{19}{0}
\@LN{20}{0}
\@LN{21}{0}
\@writefile{brf}{\backcite{he2017maskrcnn}{{1}{A}{figure.caption.2}}}
\@LN{22}{0}
\@LN{23}{0}
\@LN{24}{0}
\@LN{25}{0}
\@writefile{brf}{\backcite{cheng2022mask2former}{{1}{A}{equation.A.1}}}
\@LN{26}{0}
\@LN{27}{0}
\@LN{28}{0}
\@LN{29}{0}
\@LN{30}{0}
\@LN{31}{0}
\@LN{32}{0}
\@LN{33}{0}
\@writefile{brf}{\backcite{nguyen2022boxer}{{1}{A}{equation.A.1}}}
\@LN{34}{0}
\@LN{35}{0}
\@LN{36}{0}
\@LN{37}{0}
\@LN{38}{0}
\@LN{39}{0}
\@writefile{brf}{\backcite{li2022vitdet}{{1}{A}{equation.A.1}}}
\@LN{40}{0}
\@LN{41}{0}
\@LN{42}{0}
\@writefile{brf}{\backcite{wu2018groupnorm}{{1}{A}{equation.A.1}}}
\@LN{43}{0}
\@writefile{brf}{\backcite{lin2017focalloss}{{1}{A}{figure.caption.3}}}
\@LN{44}{0}
\@writefile{brf}{\backcite{milletari2016vnet}{{1}{A}{figure.caption.3}}}
\@LN{45}{0}
\@LN{46}{0}
\@writefile{brf}{\backcite{rezatofighi2019giou}{{1}{A}{figure.caption.3}}}
\@LN{47}{0}
\@LN{48}{0}
\@LN{49}{0}
\@LN{50}{0}
\citation{nguyen2022boxer}
\citation{li2022vitdet}
\citation{nguyen2022boxer}
\citation{nguyen2022boxer}
\citation{touvron2022deit3}
\citation{touvron2021deit}
\citation{he2022mae}
\citation{li2022vitdet}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  \textbf  {The creation of input features. Left:} The creation of feature pyramids from the last feature of the plain backbone, ViT, in SimpleFPN\nobreakspace  {}\citep  {li2022vitdet} where different stacks of convolutional layers are used to create features at different scales. \textbf  {Right:} The design of our single-scale feature map with only one layer. \relax }}{2}{figure.caption.3}\protected@file@percent }
\@writefile{brf}{\backcite{li2022vitdet}{{2}{2}{figure.caption.3}}}
\newlabel{fig:fpn_vs_ss}{{2}{2}{\textbf {The creation of input features. Left:} The creation of feature pyramids from the last feature of the plain backbone, ViT, in SimpleFPN~\citep {li2022vitdet} where different stacks of convolutional layers are used to create features at different scales. \textbf {Right:} The design of our single-scale feature map with only one layer. \relax }{figure.caption.3}{}}
\newlabel{fig:fpn_vs_ss@cref}{{[figure][2][2147483647]2}{[1][1][]2}}
\@LN@col{1}
\@LN{51}{1}
\@LN{52}{1}
\@LN{53}{1}
\@LN{54}{1}
\@LN{55}{1}
\@LN{56}{1}
\@LN{57}{1}
\@LN{58}{1}
\@writefile{brf}{\backcite{nguyen2022boxer}{{2}{A}{figure.caption.3}}}
\@LN{59}{1}
\@LN{60}{1}
\@LN{61}{1}
\@LN{62}{1}
\@LN{63}{1}
\@LN{64}{1}
\@LN{65}{1}
\@LN{66}{1}
\@LN{67}{1}
\@LN{68}{1}
\@LN{69}{1}
\@writefile{brf}{\backcite{li2022vitdet}{{2}{A}{figure.caption.3}}}
\@LN{70}{1}
\@LN{71}{1}
\@LN{72}{1}
\@LN{73}{1}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Hyper-parameters of backbone and detection head for different sizes of SimPLR\xspace  (base -- large -- huge models). Note that these settings are the same for all three tasks.\relax }}{2}{table.caption.4}\protected@file@percent }
\newlabel{tab:hyper}{{1}{2}{Hyper-parameters of backbone and detection head for different sizes of \ours (base -- large -- huge models). Note that these settings are the same for all three tasks.\relax }{table.caption.4}{}}
\newlabel{tab:hyper@cref}{{[table][1][2147483647]1}{[1][2][]2}}
\@LN@col{2}
\@LN{74}{1}
\@writefile{toc}{\contentsline {section}{\numberline {B}\hskip -1em.\nobreakspace  {}Additional Results}{2}{appendix.B}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \textbf  {More panoptic segmentation comparison} between SimPLR\xspace  with ViT-B backbone and other methods with Swin-B backbone. All backbones are pre-trained on ImageNet-1K with supervised pre-training. SimPLR\xspace  still shows competitive results when using only single-scale input.\relax }}{2}{table.caption.5}\protected@file@percent }
\newlabel{tab:more_panop}{{2}{2}{\textbf {More panoptic segmentation comparison} between \ours with ViT-B backbone and other methods with Swin-B backbone. All backbones are pre-trained on ImageNet-1K with supervised pre-training. \ours still shows competitive results when using only single-scale input.\relax }{table.caption.5}{}}
\newlabel{tab:more_panop@cref}{{[table][2][2147483647]2}{[1][2][]2}}
\@LN{75}{1}
\@LN{76}{1}
\@LN{77}{1}
\@LN{78}{1}
\@LN{79}{1}
\@LN{80}{1}
\@LN{81}{1}
\@LN{82}{1}
\@LN{83}{1}
\@LN{84}{1}
\@LN{85}{1}
\@writefile{brf}{\backcite{touvron2022deit3}{{2}{B}{table.caption.6}}}
\@writefile{brf}{\backcite{touvron2021deit}{{2}{B}{table.caption.6}}}
\@LN{86}{1}
\@LN{87}{1}
\@LN{88}{1}
\@writefile{brf}{\backcite{he2022mae}{{2}{B}{table.caption.6}}}
\@LN{89}{1}
\@LN{90}{1}
\@LN{91}{1}
\@LN{92}{1}
\@LN{93}{1}
\@writefile{brf}{\backcite{li2022vitdet}{{2}{B}{table.caption.6}}}
\@LN{94}{1}
\@LN{95}{1}
\@LN{96}{1}
\citation{deng2009imagenet}
\citation{lin2014mscoco}
\bibstyle{ieeenat_fullname}
\bibdata{simplr}
\bibcite{cheng2022mask2former}{{1}{2022}{{Cheng et~al.}}{{Cheng, Misra, Schwing, Kirillov, and Girdhar}}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces  \textbf  {Ablation on pre-training strategies} of the plain ViT backbone using SimPLR\xspace  evaluated on COCO object detection and instance segmentation. We compare the ViT backbone pre-trained using supervised methods (\emph  {top} row) \emph  {vs}\onedot  self-supervised methods (\emph  {bottom} row) with different sizes of pre-training dataset (ImageNet-1K \emph  {vs}\onedot  ImageNet-21K). Here, we use the $5\times $ schedule as in \cite  {nguyen2022boxer}. It can be seen that SimPLR\xspace  with the plain ViT backbone benefits from better pre-training approaches and with more pre-training data. \relax }}{3}{table.caption.6}\protected@file@percent }
\@writefile{brf}{\backcite{nguyen2022boxer}{{3}{3}{table.caption.6}}}
\newlabel{tab:pretrain}{{3}{3}{\textbf {Ablation on pre-training strategies} of the plain ViT backbone using \ours evaluated on COCO object detection and instance segmentation. We compare the ViT backbone pre-trained using supervised methods (\emph {top} row) \vs self-supervised methods (\emph {bottom} row) with different sizes of pre-training dataset (ImageNet-1K \vs ImageNet-21K). Here, we use the $5\times $ schedule as in \cite {nguyen2022boxer}. It can be seen that \ours with the plain ViT backbone benefits from better pre-training approaches and with more pre-training data. \relax }{table.caption.6}{}}
\newlabel{tab:pretrain@cref}{{[table][3][2147483647]3}{[1][2][]3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Qualitative results} for object detection, instance segmentation, and panoptic segmentation generated by SimPLR\xspace  using ViT-B as backbone on the COCO {\texttt  {val}}\xspace  set. In each pair, the left image shows the visualization of object detection and instance segmentation, while the right one indicates the panoptic segmentation prediction.\relax }}{3}{figure.caption.7}\protected@file@percent }
\newlabel{fig:vis_sup}{{3}{3}{\textbf {Qualitative results} for object detection, instance segmentation, and panoptic segmentation generated by \ours using ViT-B as backbone on the COCO \val set. In each pair, the left image shows the visualization of object detection and instance segmentation, while the right one indicates the panoptic segmentation prediction.\relax }{figure.caption.7}{}}
\newlabel{fig:vis_sup@cref}{{[figure][3][2147483647]3}{[1][2][]3}}
\@LN@col{1}
\@LN{97}{2}
\@writefile{toc}{\contentsline {section}{\numberline {C}\hskip -1em.\nobreakspace  {}Qualitative results}{3}{appendix.C}\protected@file@percent }
\@LN{98}{2}
\@LN{99}{2}
\@LN{100}{2}
\@LN@col{2}
\@LN{101}{2}
\@writefile{toc}{\contentsline {section}{\numberline {D}\hskip -1em.\nobreakspace  {}Asset Licenses}{3}{appendix.D}\protected@file@percent }
\@writefile{brf}{\backcite{deng2009imagenet}{{3}{D}{appendix.D}}}
\@writefile{brf}{\backcite{lin2014mscoco}{{3}{D}{appendix.D}}}
\@LN{102}{2}
\@LN{103}{2}
\@LN{104}{2}
\@LN{105}{2}
\bibcite{deng2009imagenet}{{2}{2009}{{Deng et~al.}}{{Deng, Dong, Socher, Li, Li, and Fei-Fei}}}
\bibcite{he2017maskrcnn}{{3}{2017}{{He et~al.}}{{He, Gkioxari, Doll√°r, and Girshick}}}
\bibcite{he2022mae}{{4}{2022}{{He et~al.}}{{He, Chen, Xie, Li, Doll{\'a}r, and Girshick}}}
\bibcite{li2022vitdet}{{5}{2022}{{Li et~al.}}{{Li, Mao, Girshick, and He}}}
\bibcite{lin2014mscoco}{{6}{2014}{{Lin et~al.}}{{Lin, Maire, Belongie, Bourdev, Girshick, Hays, Perona, Ramanan, Doll{\'{a}}r, and Zitnick}}}
\bibcite{lin2017focalloss}{{7}{2017}{{Lin et~al.}}{{Lin, Goyal, Girshick, He, and Doll{\'{a}}r}}}
\bibcite{milletari2016vnet}{{8}{2016}{{Milletari et~al.}}{{Milletari, Navab, and Ahmadi}}}
\bibcite{nguyen2022boxer}{{9}{2022}{{Nguyen et~al.}}{{Nguyen, Ju, Booij, Oswald, and Snoek}}}
\bibcite{rezatofighi2019giou}{{10}{2019}{{Rezatofighi et~al.}}{{Rezatofighi, Tsoi, Gwak, Sadeghian, Reid, and Savarese}}}
\bibcite{touvron2021deit}{{11}{2021}{{Touvron et~al.}}{{Touvron, Cord, Douze, Massa, Sablayrolles, and Jegou}}}
\bibcite{touvron2022deit3}{{12}{2022}{{Touvron et~al.}}{{Touvron, Cord, and Jegou}}}
\bibcite{wu2018groupnorm}{{13}{2018}{{Wu and He}}{{}}}
\@LN@col{1}
\@LN{106}{3}
\@LN{107}{3}
\@LN{108}{3}
\@LN{109}{3}
\@LN{110}{3}
\@LN{111}{3}
\@LN{112}{3}
\@LN{113}{3}
\@LN{114}{3}
\@LN{115}{3}
\@LN{116}{3}
\@LN{117}{3}
\@LN{118}{3}
\@LN{119}{3}
\@LN{120}{3}
\@LN{121}{3}
\@LN{122}{3}
\@LN{123}{3}
\@LN{124}{3}
\@LN{125}{3}
\@LN{126}{3}
\@LN{127}{3}
\@LN{128}{3}
\@LN{129}{3}
\@LN{130}{3}
\@LN{131}{3}
\@LN{132}{3}
\@LN{133}{3}
\@LN{134}{3}
\@LN{135}{3}
\@LN{136}{3}
\@LN{137}{3}
\@LN{138}{3}
\@LN{139}{3}
\@LN{140}{3}
\@LN{141}{3}
\@LN{142}{3}
\@LN{143}{3}
\@LN{144}{3}
\@LN{145}{3}
\@LN@col{2}
\gdef \@abspage@last{4}
