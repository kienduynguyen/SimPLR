\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{vaswani2017transformer}
\citation{liu2021swintransformer}
\citation{dosovitskiy2021vit}
\citation{nicolas2020detr}
\citation{zhu2021deformable}
\citation{nguyen2022boxer}
\citation{zhang2021knet}
\citation{cheng2022mask2former}
\citation{brown2020gpt3}
\citation{devlin2019bert}
\citation{liu2021swintransformer}
\citation{fan2021mvit}
\citation{wang2021pvit}
\citation{heo2021rethinkingvit}
\citation{tsung2017fpn}
\citation{nicolas2020detr}
\citation{dosovitskiy2021vit}
\citation{he2022mae}
\citation{bao2022beit}
\citation{zhai2022scalingvit}
\citation{dehghani2022scalingvit22b}
\citation{dosovitskiy2021vit}
\citation{li2022vitdet}
\citation{ren2015faster_rcnn}
\citation{he2017maskrcnn}
\citation{zhu2021deformable}
\citation{nguyen2022boxer}
\citation{cheng2022mask2former}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:introduction}{{1}{1}{Introduction}{section.1}{}}
\newlabel{sec:introduction@cref}{{[section][1][]1}{[1][1][]1}}
\citation{li2022vitdet}
\citation{li2022vitdet}
\citation{nguyen2022boxer}
\citation{cheng2022mask2former}
\citation{liu2021swintransformer}
\citation{li2022vitdet}
\citation{nguyen2022boxer}
\citation{cheng2022mask2former}
\citation{liu2021swintransformer}
\citation{li2022vitdet}
\citation{nguyen2022boxer}
\citation{nguyen2022boxer}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  \textbf  {Object detection architectures. Left:} The plain-backbone detector from \cite  {li2022vitdet} whose input (denoted in dashed region) are multi-scale features. \textbf  {Middle:} state-of-the-art end-to-end detectors \cite  {nguyen2022boxer,cheng2022mask2former} utilize a hierarchical backbone (\emph  {i.e}\onedot  , SwinTransformer \cite  {liu2021swintransformer}) to create multi-scale inputs. \textbf  {Right:} Our simple single-scale detector following the end-to-end framework. Where existing detectors require multi-scale feature maps to be effective, we propose a plain detector whose backbone and detection head require only a single-scale feature. \relax }}{2}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:compare}{{1}{2}{\textbf {Object detection architectures. Left:} The plain-backbone detector from \cite {li2022vitdet} whose input (denoted in dashed region) are multi-scale features. \textbf {Middle:} state-of-the-art end-to-end detectors \cite {nguyen2022boxer,cheng2022mask2former} utilize a hierarchical backbone (\ie , SwinTransformer \cite {liu2021swintransformer}) to create multi-scale inputs. \textbf {Right:} Our simple single-scale detector following the end-to-end framework. Where existing detectors require multi-scale feature maps to be effective, we propose a plain detector whose backbone and detection head require only a single-scale feature. \relax }{figure.caption.2}{}}
\newlabel{fig:compare@cref}{{[figure][1][]1}{[1][2][]2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Method}{2}{section.2}\protected@file@percent }
\newlabel{sec:mpm}{{2}{2}{Method}{section.2}{}}
\newlabel{sec:mpm@cref}{{[section][2][]2}{[1][2][]2}}
\citation{nguyen2022boxer}
\citation{vaswani2017transformer}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  \textbf  {Scale-aware attention mechanism. Left:} The fixed-scale attention with attention computation of a single scale per head. \textbf  {Right:} The adaptive-scale attention with reference windows of multiple scales in each attention head. The 2-scale reference windows (denoted in yellow and orange) are used in this case. Unlike fixed-scale attention that transforms a single-scale reference window, adaptive-scale attention generates multi-scale regions of interest which are then selected using attention weights. \relax }}{3}{figure.caption.3}\protected@file@percent }
\newlabel{fig:scale_aware_attn}{{2}{3}{\textbf {Scale-aware attention mechanism. Left:} The fixed-scale attention with attention computation of a single scale per head. \textbf {Right:} The adaptive-scale attention with reference windows of multiple scales in each attention head. The 2-scale reference windows (denoted in yellow and orange) are used in this case. Unlike fixed-scale attention that transforms a single-scale reference window, adaptive-scale attention generates multi-scale regions of interest which are then selected using attention weights. \relax }{figure.caption.3}{}}
\newlabel{fig:scale_aware_attn@cref}{{[figure][2][]2}{[1][3][]3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Background}{3}{subsection.2.1}\protected@file@percent }
\newlabel{eq:multihead}{{4}{3}{Background}{equation.2.4}{}}
\newlabel{eq:multihead@cref}{{[equation][4][]4}{[1][3][]3}}
\citation{nguyen2022boxer}
\citation{pinheiro2016refineobject}
\citation{ren2015faster_rcnn}
\citation{tsung2017fpn}
\citation{zhu2021deformable}
\citation{nguyen2022boxer}
\citation{li2022vitdet}
\citation{wei2016ssd}
\citation{tsung2017fpn}
\citation{zhu2021deformable}
\citation{li2022vitdet}
\citation{chen2022uvit}
\citation{vaswani2017transformer}
\citation{chen2022uvit}
\citation{zhu2021deformable}
\citation{ren2015faster_rcnn}
\citation{ren2015faster_rcnn}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}SimPLR\xspace  : A Simple and Plain Single-Scale Detector}{4}{subsection.2.2}\protected@file@percent }
\newlabel{sec:single_scale}{{2.2}{4}{\ours : A Simple and Plain Single-Scale Detector}{subsection.2.2}{}}
\newlabel{sec:single_scale@cref}{{[subsection][2][2]2.2}{[1][4][]4}}
\citation{li2022vitdet}
\citation{nicolas2020detr}
\citation{nguyen2022boxer}
\citation{nguyen2022boxer}
\citation{cheng2022mask2former}
\citation{cheng2022mask2former}
\citation{nguyen2022boxer}
\citation{nguyen2022boxer}
\citation{he2017maskrcnn}
\citation{cheng2022mask2former}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}SimPLR for Universal Detection and Segmentation}{5}{subsection.2.3}\protected@file@percent }
\citation{lin2014mscoco}
\citation{li2022vitdet}
\citation{lin2014mscoco}
\citation{li2022vitdet}
\citation{li2022vitdet}
\citation{nguyen2022boxer}
\citation{lin2014mscoco}
\citation{li2022vitdet}
\citation{lin2014mscoco}
\citation{li2022vitdet}
\citation{li2022vitdet}
\citation{nguyen2022boxer}
\citation{lin2014mscoco}
\citation{nguyen2022boxer}
\citation{lin2014mscoco}
\citation{nguyen2022boxer}
\citation{lin2014mscoco}
\citation{he2022mae}
\citation{nguyen2022boxer}
\citation{loshchilov2019adamw}
\citation{dosovitskiy2021vit}
\citation{shiasi2021lsjitter}
\citation{shiasi2021lsjitter}
\citation{li2022vitdet}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \textbf  {Ablation on single-scale \emph  {vs}\onedot  multi-scale feature map} on COCO\nobreakspace  {}\cite  {lin2014mscoco} object detection and instance segmentation using ViT-B as backbone. Compared to ViTDet, BoxeR performs better for both tasks with less FLOPs. Interestingly, SimPLR\xspace  performs on par with BoxeR under the same FLOPs despite using only single-scale input ($\ddag  $: BoxeR with SimpleFPN \cite  {li2022vitdet}).\relax }}{6}{table.caption.4}\protected@file@percent }
\newlabel{tab:compare}{{1}{6}{\textbf {Ablation on single-scale \vs multi-scale feature map} on COCO~\cite {lin2014mscoco} object detection and instance segmentation using ViT-B as backbone. Compared to ViTDet, BoxeR performs better for both tasks with less FLOPs. Interestingly, \ours performs on par with BoxeR under the same FLOPs despite using only single-scale input ($\ddag $: BoxeR with SimpleFPN \cite {li2022vitdet}).\relax }{table.caption.4}{}}
\newlabel{tab:compare@cref}{{[table][1][]1}{[1][6][]6}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Experiments}{6}{section.3}\protected@file@percent }
\newlabel{sec:experiments}{{3}{6}{Experiments}{section.3}{}}
\newlabel{sec:experiments@cref}{{[section][3][]3}{[1][6][]6}}
\newlabel{tab:strat}{{2a}{6}{Subtable 2a}{subtable.2.1}{}}
\newlabel{sub@tab:strat}{{(a)}{a}{Subtable 2a\relax }{subtable.2.1}{}}
\newlabel{tab:strat@cref}{{[subtable][1][2]2a}{[1][6][]6}}
\newlabel{tab:feat_scale}{{2b}{6}{Subtable 2b}{subtable.2.2}{}}
\newlabel{sub@tab:feat_scale}{{(b)}{b}{Subtable 2b\relax }{subtable.2.2}{}}
\newlabel{tab:feat_scale@cref}{{[subtable][2][2]2b}{[1][6][]6}}
\newlabel{tab:window_size}{{2c}{6}{Subtable 2c}{subtable.2.3}{}}
\newlabel{sub@tab:window_size}{{(c)}{c}{Subtable 2c\relax }{subtable.2.3}{}}
\newlabel{tab:window_size@cref}{{[subtable][3][2]2c}{[1][6][]6}}
\newlabel{tab:num_scale}{{2d}{6}{Subtable 2d}{subtable.2.4}{}}
\newlabel{sub@tab:num_scale}{{(d)}{d}{Subtable 2d\relax }{subtable.2.4}{}}
\newlabel{tab:num_scale@cref}{{[subtable][4][2]2d}{[1][6][]6}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces  \textbf  {Ablation on the design of attention mechanism} using a plain ViT backbone on COCO\nobreakspace  {}\cite  {lin2014mscoco} object detection and instance segmentation. SimPLR\xspace  receives the single-scale input from ViT and makes predictions. Compared to the na\"ive baseline which employs BoxeR and box-attention \cite  {nguyen2022boxer} with single-scale features, SimPLR\xspace  improves performance of the plain detector.\relax }}{6}{table.caption.5}\protected@file@percent }
\@writefile{lot}{\contentsline {subtable}{\numberline{(a)}{\ignorespaces { \textbf {Strategy for learning scale equivariance.}}}}{6}{subtable.2.1}\protected@file@percent }
\@writefile{lot}{\contentsline {subtable}{\numberline{(b)}{\ignorespaces { \textbf {Scales of input feature map.}}}}{6}{subtable.2.2}\protected@file@percent }
\@writefile{lot}{\contentsline {subtable}{\numberline{(c)}{\ignorespaces { \textbf {Reference window size.}}}}{6}{subtable.2.3}\protected@file@percent }
\@writefile{lot}{\contentsline {subtable}{\numberline{(d)}{\ignorespaces { \textbf {Number of window scales}.}}}{6}{subtable.2.4}\protected@file@percent }
\newlabel{tab:det_ablation}{{2}{6}{\textbf {Ablation on the design of attention mechanism} using a plain ViT backbone on COCO~\cite {lin2014mscoco} object detection and instance segmentation. \ours receives the single-scale input from ViT and makes predictions. Compared to the na\"ive baseline which employs BoxeR and box-attention \cite {nguyen2022boxer} with single-scale features, \ours improves performance of the plain detector.\relax }{table.caption.5}{}}
\newlabel{tab:det_ablation@cref}{{[table][2][]2}{[1][6][]6}}
\citation{nguyen2022boxer}
\citation{lin2014mscoco}
\citation{lin2014mscoco}
\citation{cai2019cascadercnn}
\citation{li2022vitdet}
\citation{zhu2021deformable}
\citation{cheng2022mask2former}
\citation{chen2022uvit}
\citation{lin2014mscoco}
\citation{lin2014mscoco}
\citation{nguyen2022boxer}
\citation{nicolas2020detr}
\newlabel{tab:attn}{{3a}{7}{Subtable 3a}{subtable.3.1}{}}
\newlabel{sub@tab:attn}{{(a)}{a}{Subtable 3a\relax }{subtable.3.1}{}}
\newlabel{tab:attn@cref}{{[subtable][1][3]3a}{[1][7][]7}}
\newlabel{tab:pdecoder}{{3b}{7}{Subtable 3b}{subtable.3.2}{}}
\newlabel{sub@tab:pdecoder}{{(b)}{b}{Subtable 3b\relax }{subtable.3.2}{}}
\newlabel{tab:pdecoder@cref}{{[subtable][2][3]3b}{[1][7][]7}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces  \textbf  {Ablation on mask prediction adaptation} in a plain detector on COCO\nobreakspace  {}\cite  {lin2014mscoco} panoptic segmentation. The masked instance-attention along with high-resolution layers improve the performance of SimPLR\xspace  on panoptic segmentation, showing its effectiveness in capturing fine-grained information.\relax }}{7}{table.caption.6}\protected@file@percent }
\@writefile{lot}{\contentsline {subtable}{\numberline{(a)}{\ignorespaces { {\bf Instance-attention \emph {vs}\onedot Masked instance-attention.} }}}{7}{subtable.3.1}\protected@file@percent }
\@writefile{lot}{\contentsline {subtable}{\numberline{(b)}{\ignorespaces { {\bf Number of high-resolution layers.}}}}{7}{subtable.3.2}\protected@file@percent }
\newlabel{tab:pan_ablation}{{3}{7}{\textbf {Ablation on mask prediction adaptation} in a plain detector on COCO~\cite {lin2014mscoco} panoptic segmentation. The masked instance-attention along with high-resolution layers improve the performance of \ours on panoptic segmentation, showing its effectiveness in capturing fine-grained information.\relax }{table.caption.6}{}}
\newlabel{tab:pan_ablation@cref}{{[table][3][]3}{[1][7][]7}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Qualitative results} for object detection and panoptic segmentation on the COCO\nobreakspace  {}\cite  {lin2014mscoco} 2017 {\texttt  {val}}\xspace  set generated by SimPLR\xspace  . Note that SimPLR\xspace  gives good predictions on small objects.\relax }}{7}{figure.caption.7}\protected@file@percent }
\newlabel{fig:vis}{{3}{7}{\textbf {Qualitative results} for object detection and panoptic segmentation on the COCO~\cite {lin2014mscoco} 2017 \val set generated by \ours . Note that \ours gives good predictions on small objects.\relax }{figure.caption.7}{}}
\newlabel{fig:vis@cref}{{[figure][3][]3}{[1][7][]7}}
\citation{nguyen2022boxer}
\citation{liu2021swintransformer}
\citation{fan2021mvit}
\citation{cheng2021maskformer}
\citation{cheng2022mask2former}
\citation{li2022vitdet}
\citation{nguyen2022boxer}
\citation{chen2022uvit}
\citation{lin2014mscoco}
\citation{li2022vitdet}
\citation{lin2014mscoco}
\citation{li2022vitdet}
\citation{li2022vitdet}
\citation{nguyen2022boxer}
\citation{cheng2022mask2former}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces \textbf  { Universal detection and segmentation comparison} for object detection, instance segmentation, and panoptic segmentation on the COCO\nobreakspace  {}\cite  {lin2014mscoco} {\texttt  {val}}\xspace  set. By default, all backbones are pre-trained using ImageNet1K. ($\dag  $: backbones pre-trained on ImageNet-22K; $\ddag  $: BoxeR with SimpleFPN from \cite  {li2022vitdet}; $5\times $: methods fine-tuned using $5\times $ schedule; n/a: method is not applicable to the task). Methods in \textcolor {gray}{gray color} are with a convolution-based detection head. SimPLR\xspace  as a plain detector demonstrates competitive performance compared to many specialized or end-to-end models with a small amount of computation and is the only one suited for all three tasks.\relax }}{8}{table.caption.8}\protected@file@percent }
\newlabel{tab:det_main}{{4}{8}{\textbf { Universal detection and segmentation comparison} for object detection, instance segmentation, and panoptic segmentation on the COCO~\cite {lin2014mscoco} \val set. By default, all backbones are pre-trained using ImageNet1K. ($\dag $: backbones pre-trained on ImageNet-22K; $\ddag $: BoxeR with SimpleFPN from \cite {li2022vitdet}; $5\times $: methods fine-tuned using $5\times $ schedule; n/a: method is not applicable to the task). Methods in \textcolor {gray}{gray color} are with a convolution-based detection head. \ours as a plain detector demonstrates competitive performance compared to many specialized or end-to-end models with a small amount of computation and is the only one suited for all three tasks.\relax }{table.caption.8}{}}
\newlabel{tab:det_main@cref}{{[table][4][]4}{[1][8][]8}}
\citation{girshick2014rcnn}
\citation{lecun95convolutional}
\citation{deng2009imagenet}
\citation{simonyan2015vgg}
\citation{xie2017resnext}
\citation{kaiming2016resnet}
\citation{huang2017densenet}
\citation{chen2020simclr}
\citation{he2022mae}
\citation{brown2020gpt3}
\citation{devlin2019bert}
\citation{chen2020igpt}
\citation{dosovitskiy2021vit}
\citation{dosovitskiy2021vit}
\citation{sermanet2014overfeat}
\citation{uijlings2013selective}
\citation{girshick2014rcnn}
\citation{ren2015faster_rcnn}
\citation{wei2016ssd}
\citation{tsung2017fpn}
\citation{li2022vitdet}
\citation{chen2022uvit}
\citation{li2022vitdet}
\citation{nicolas2020detr}
\citation{dong2021solq}
\citation{nguyen2022boxer}
\citation{cheng2021maskformer}
\citation{zhang2021knet}
\citation{cheng2022mask2former}
\citation{yu2022kmmask}
\bibstyle{ieee_fullname}
\bibdata{ssd}
\@writefile{toc}{\contentsline {section}{\numberline {4}Related Work}{9}{section.4}\protected@file@percent }
\newlabel{sec:related_work}{{4}{9}{Related Work}{section.4}{}}
\newlabel{sec:related_work@cref}{{[section][4][]4}{[1][9][]9}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{9}{section.5}\protected@file@percent }
\newlabel{sec:conclusion}{{5}{9}{Conclusion}{section.5}{}}
\newlabel{sec:conclusion@cref}{{[section][5][]5}{[1][9][]9}}
\bibcite{bao2022beit}{1}
\bibcite{brown2020gpt3}{2}
\bibcite{cai2019cascadercnn}{3}
\bibcite{nicolas2020detr}{4}
\bibcite{chen2020igpt}{5}
\bibcite{chen2020simclr}{6}
\bibcite{chen2022uvit}{7}
\bibcite{cheng2022mask2former}{8}
\bibcite{cheng2021maskformer}{9}
\bibcite{dehghani2022scalingvit22b}{10}
\bibcite{deng2009imagenet}{11}
\bibcite{devlin2019bert}{12}
\bibcite{dong2021solq}{13}
\bibcite{dosovitskiy2021vit}{14}
\bibcite{fan2021mvit}{15}
\bibcite{shiasi2021lsjitter}{16}
\bibcite{girshick2014rcnn}{17}
\bibcite{he2022mae}{18}
\bibcite{he2017maskrcnn}{19}
\bibcite{kaiming2016resnet}{20}
\bibcite{heo2021rethinkingvit}{21}
\bibcite{huang2017densenet}{22}
\bibcite{lecun95convolutional}{23}
\bibcite{li2022vitdet}{24}
\bibcite{lin2014mscoco}{25}
\bibcite{tsung2017fpn}{26}
\bibcite{wei2016ssd}{27}
\bibcite{liu2021swintransformer}{28}
\bibcite{loshchilov2019adamw}{29}
\bibcite{nguyen2022boxer}{30}
\bibcite{pinheiro2016refineobject}{31}
\bibcite{ren2015faster_rcnn}{32}
\bibcite{sermanet2014overfeat}{33}
\bibcite{simonyan2015vgg}{34}
\bibcite{uijlings2013selective}{35}
\bibcite{vaswani2017transformer}{36}
\bibcite{wang2021pvit}{37}
\bibcite{xie2017resnext}{38}
\bibcite{yu2022kmmask}{39}
\bibcite{zhai2022scalingvit}{40}
\bibcite{zhang2021knet}{41}
\bibcite{zhu2021deformable}{42}
\gdef \@abspage@last{11}
