\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{vaswani2017transformer}
\citation{liu2021swintransformer,dosovitskiy2021vit}
\citation{nicolas2020detr,zhu2021deformable,nguyen2022boxer}
\citation{zhang2021knet,cheng2022mask2former}
\citation{brown2020gpt3,devlin2019bert}
\citation{liu2021swintransformer}
\citation{fan2021mvit,wang2021pvit,heo2021rethinkingvit}
\citation{tsung2017fpn}
\citation{nicolas2020detr}
\citation{dosovitskiy2021vit}
\citation{he2022mae,bao2022beit,zhai2022scalingvit,dehghani2022scalingvit22b}
\citation{dosovitskiy2021vit,li2022vitdet}
\citation{ren2015faster_rcnn,he2017maskrcnn}
\citation{zhu2021deformable,nguyen2022boxer,cheng2022mask2former}
\citation{li2022vitdet}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.\nobreakspace  {}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{\hskip -1em.~Introduction}{section.1}{}}
\newlabel{sec:intro@cref}{{[section][1][]1}{[1][1][]1}}
\@writefile{brf}{\backcite{vaswani2017transformer}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{liu2021swintransformer,dosovitskiy2021vit}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{nicolas2020detr,zhu2021deformable,nguyen2022boxer}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{zhang2021knet,cheng2022mask2former}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{brown2020gpt3,devlin2019bert}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{liu2021swintransformer}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{fan2021mvit,wang2021pvit,heo2021rethinkingvit}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{tsung2017fpn}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{nicolas2020detr}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{dosovitskiy2021vit}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{he2022mae,bao2022beit,zhai2022scalingvit,dehghani2022scalingvit22b}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{dosovitskiy2021vit,li2022vitdet}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{ren2015faster_rcnn,he2017maskrcnn}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{zhu2021deformable,nguyen2022boxer,cheng2022mask2former}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{li2022vitdet}{{1}{1}{section.1}}}
\citation{li2022vitdet}
\citation{nguyen2022boxer,cheng2022mask2former}
\citation{liu2021swintransformer}
\citation{li2022vitdet}
\citation{nguyen2022boxer,cheng2022mask2former}
\citation{liu2021swintransformer}
\citation{girshick2014rcnn}
\citation{lecun95convolutional}
\citation{deng2009imagenet}
\citation{simonyan2015vgg,xie2017resnext,kaiming2016resnet,huang2017densenet}
\citation{chen2020simclr,he2022mae}
\citation{brown2020gpt3,devlin2019bert}
\citation{chen2020igpt,dosovitskiy2021vit}
\citation{dosovitskiy2021vit}
\citation{sermanet2014overfeat}
\citation{uijlings2013selective}
\citation{girshick2014rcnn}
\citation{ren2015faster_rcnn}
\citation{wei2016ssd}
\citation{tsung2017fpn}
\citation{li2022vitdet,chen2022uvit}
\citation{li2022vitdet}
\citation{nicolas2020detr}
\citation{dong2021solq,nguyen2022boxer}
\citation{cheng2021maskformer}
\citation{zhang2021knet}
\citation{cheng2022mask2former}
\citation{yu2022kmmask}
\citation{li2022vitdet,nguyen2022boxer}
\citation{nguyen2022boxer}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  \textbf  {Object detection architectures. Left:} The plain-backbone detector from \cite  {li2022vitdet} whose input (denoted in dashed region) are multi-scale features. \textbf  {Middle:} state-of-the-art end-to-end detectors \cite  {nguyen2022boxer,cheng2022mask2former} utilize a hierarchical backbone (\emph  {i.e}\onedot  , SwinTransformer \cite  {liu2021swintransformer}) to create multi-scale inputs. \textbf  {Right:} Our simple single-scale detector following the end-to-end framework. Where existing detectors require multi-scale feature maps to be effective, we propose a plain detector whose backbone and detection head require only a single-scale feature. \relax }}{2}{figure.caption.1}\protected@file@percent }
\@writefile{brf}{\backcite{li2022vitdet}{{2}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{nguyen2022boxer,cheng2022mask2former}{{2}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{liu2021swintransformer}{{2}{1}{figure.caption.1}}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:compare}{{1}{2}{\textbf {Object detection architectures. Left:} The plain-backbone detector from \cite {li2022vitdet} whose input (denoted in dashed region) are multi-scale features. \textbf {Middle:} state-of-the-art end-to-end detectors \cite {nguyen2022boxer,cheng2022mask2former} utilize a hierarchical backbone (\ie , SwinTransformer \cite {liu2021swintransformer}) to create multi-scale inputs. \textbf {Right:} Our simple single-scale detector following the end-to-end framework. Where existing detectors require multi-scale feature maps to be effective, we propose a plain detector whose backbone and detection head require only a single-scale feature. \relax }{figure.caption.1}{}}
\newlabel{fig:compare@cref}{{[figure][1][]1}{[1][2][]2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.\nobreakspace  {}Related Work}{2}{section.2}\protected@file@percent }
\newlabel{sec:related_work}{{2}{2}{\hskip -1em.~Related Work}{section.2}{}}
\newlabel{sec:related_work@cref}{{[section][2][]2}{[1][2][]2}}
\@writefile{brf}{\backcite{girshick2014rcnn}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{lecun95convolutional}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{deng2009imagenet}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{simonyan2015vgg,xie2017resnext,kaiming2016resnet,huang2017densenet}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{chen2020simclr,he2022mae}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{brown2020gpt3,devlin2019bert}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{chen2020igpt,dosovitskiy2021vit}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{dosovitskiy2021vit}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{sermanet2014overfeat}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{uijlings2013selective}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{girshick2014rcnn}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{ren2015faster_rcnn}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{wei2016ssd}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{tsung2017fpn}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{li2022vitdet,chen2022uvit}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{li2022vitdet}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{nicolas2020detr}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{dong2021solq,nguyen2022boxer}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{cheng2021maskformer}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{zhang2021knet}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{cheng2022mask2former}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{yu2022kmmask}{{2}{2}{section.2}}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.\nobreakspace  {}Method}{2}{section.3}\protected@file@percent }
\newlabel{sec:mpm}{{3}{2}{\hskip -1em.~Method}{section.3}{}}
\newlabel{sec:mpm@cref}{{[section][3][]3}{[1][2][]2}}
\@writefile{brf}{\backcite{li2022vitdet,nguyen2022boxer}{{2}{3}{section.3}}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{2}{3}{section.3}}}
\citation{nguyen2022boxer}
\citation{vaswani2017transformer}
\citation{nguyen2022boxer}
\citation{pinheiro2016refineobject,ren2015faster_rcnn,tsung2017fpn}
\citation{zhu2021deformable,nguyen2022boxer,li2022vitdet}
\citation{wei2016ssd,tsung2017fpn,zhu2021deformable}
\citation{li2022vitdet,chen2022uvit}
\citation{vaswani2017transformer}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  \textbf  {Scale-aware attention mechanism. Left:} The fixed-scale attention with attention computation of a single scale per head. \textbf  {Right:} The adaptive-scale attention with reference windows of multiple scales in each attention head. The 2-scale reference windows (denoted in yellow and orange) are used in this case. Unlike fixed-scale attention that transforms a single-scale reference window, adaptive-scale attention generates multi-scale regions of interest which are then selected using attention weights. \relax }}{3}{figure.caption.2}\protected@file@percent }
\newlabel{fig:scale_aware_attn}{{2}{3}{\textbf {Scale-aware attention mechanism. Left:} The fixed-scale attention with attention computation of a single scale per head. \textbf {Right:} The adaptive-scale attention with reference windows of multiple scales in each attention head. The 2-scale reference windows (denoted in yellow and orange) are used in this case. Unlike fixed-scale attention that transforms a single-scale reference window, adaptive-scale attention generates multi-scale regions of interest which are then selected using attention weights. \relax }{figure.caption.2}{}}
\newlabel{fig:scale_aware_attn@cref}{{[figure][2][]2}{[1][3][]3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}\hskip -1em.\nobreakspace  {}Background}{3}{subsection.3.1}\protected@file@percent }
\@writefile{brf}{\backcite{nguyen2022boxer}{{3}{3.1}{figure.caption.2}}}
\@writefile{brf}{\backcite{vaswani2017transformer}{{3}{3.1}{equation.3.3}}}
\newlabel{eq:multihead}{{4}{3}{\hskip -1em.~Background}{equation.3.4}{}}
\newlabel{eq:multihead@cref}{{[equation][4][]4}{[1][3][]3}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{3}{3.1}{equation.3.6}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}\hskip -1em.\nobreakspace  {}BoxeR\xspace  : A Simple and Plain Single-Scale Detector}{3}{subsection.3.2}\protected@file@percent }
\newlabel{sec:single_scale}{{3.2}{3}{\hskip -1em.~\ours : A Simple and Plain Single-Scale Detector}{subsection.3.2}{}}
\newlabel{sec:single_scale@cref}{{[subsection][2][3]3.2}{[1][3][]3}}
\@writefile{brf}{\backcite{pinheiro2016refineobject,ren2015faster_rcnn,tsung2017fpn}{{3}{3.2}{subsection.3.2}}}
\@writefile{brf}{\backcite{zhu2021deformable,nguyen2022boxer,li2022vitdet}{{3}{3.2}{subsection.3.2}}}
\@writefile{brf}{\backcite{wei2016ssd,tsung2017fpn,zhu2021deformable}{{3}{3.2}{subsection.3.2}}}
\@writefile{brf}{\backcite{li2022vitdet,chen2022uvit}{{3}{3.2}{subsection.3.2}}}
\citation{chen2022uvit}
\citation{zhu2021deformable}
\citation{ren2015faster_rcnn}
\citation{ren2015faster_rcnn}
\citation{li2022vitdet}
\citation{nicolas2020detr,nguyen2022boxer}
\citation{nguyen2022boxer}
\citation{cheng2022mask2former}
\citation{cheng2022mask2former}
\citation{nguyen2022boxer}
\@writefile{brf}{\backcite{vaswani2017transformer}{{4}{3.2}{subsection.3.2}}}
\@writefile{brf}{\backcite{chen2022uvit}{{4}{3.2}{subsection.3.2}}}
\@writefile{brf}{\backcite{zhu2021deformable}{{4}{3.2}{subsection.3.2}}}
\@writefile{brf}{\backcite{ren2015faster_rcnn}{{4}{3.2}{subsection.3.2}}}
\@writefile{brf}{\backcite{ren2015faster_rcnn}{{4}{3.2}{subsection.3.2}}}
\@writefile{brf}{\backcite{li2022vitdet}{{4}{3.2}{Item.2}}}
\@writefile{brf}{\backcite{nicolas2020detr,nguyen2022boxer}{{4}{3.2}{Item.2}}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{4}{3.2}{Item.2}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}\hskip -1em.\nobreakspace  {}SimPLR for Universal Detection and Segmentation}{4}{subsection.3.3}\protected@file@percent }
\citation{nguyen2022boxer}
\citation{he2017maskrcnn}
\citation{cheng2022mask2former}
\citation{lin2014mscoco}
\citation{li2022vitdet}
\citation{lin2014mscoco}
\citation{li2022vitdet}
\citation{li2022vitdet}
\citation{nguyen2022boxer}
\citation{lin2014mscoco}
\citation{nguyen2022boxer}
\citation{lin2014mscoco}
\citation{nguyen2022boxer}
\citation{lin2014mscoco}
\citation{he2022mae}
\citation{nguyen2022boxer}
\citation{loshchilov2019adamw}
\citation{dosovitskiy2021vit}
\citation{shiasi2021lsjitter}
\citation{shiasi2021lsjitter}
\citation{li2022vitdet}
\@writefile{brf}{\backcite{cheng2022mask2former}{{5}{3.3}{subsection.3.3}}}
\@writefile{brf}{\backcite{cheng2022mask2former}{{5}{3.3}{subsection.3.3}}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{5}{3.3}{subsection.3.3}}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{5}{3.3}{subsection.3.3}}}
\@writefile{brf}{\backcite{he2017maskrcnn}{{5}{3.3}{subsection.3.3}}}
\@writefile{brf}{\backcite{cheng2022mask2former}{{5}{3.3}{equation.3.9}}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \textbf  {Ablation on single-scale \emph  {vs}\onedot  multi-scale feature map} on COCO\nobreakspace  {}\cite  {lin2014mscoco} object detection and instance segmentation using ViT-B as backbone. Compared to ViTDet, BoxeR performs better for both tasks with less FLOPs. Interestingly, BoxeR\xspace  performs on par with BoxeR under the same FLOPs despite using only single-scale input ($\ddag  $: BoxeR with SimpleFPN \cite  {li2022vitdet}).\relax }}{5}{table.caption.3}\protected@file@percent }
\@writefile{brf}{\backcite{lin2014mscoco}{{5}{1}{table.caption.3}}}
\@writefile{brf}{\backcite{li2022vitdet}{{5}{1}{table.caption.3}}}
\newlabel{tab:compare}{{1}{5}{\textbf {Ablation on single-scale \vs multi-scale feature map} on COCO~\cite {lin2014mscoco} object detection and instance segmentation using ViT-B as backbone. Compared to ViTDet, BoxeR performs better for both tasks with less FLOPs. Interestingly, \ours performs on par with BoxeR under the same FLOPs despite using only single-scale input ($\ddag $: BoxeR with SimpleFPN \cite {li2022vitdet}).\relax }{table.caption.3}{}}
\newlabel{tab:compare@cref}{{[table][1][]1}{[1][5][]5}}
\@writefile{brf}{\backcite{li2022vitdet}{{5}{\caption@xref {??}{ on input line 44}}{table.caption.3}}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{5}{\caption@xref {??}{ on input line 45}}{table.caption.3}}}
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.\nobreakspace  {}Experiments}{5}{section.4}\protected@file@percent }
\newlabel{sec:experiments}{{4}{5}{\hskip -1em.~Experiments}{section.4}{}}
\newlabel{sec:experiments@cref}{{[section][4][]4}{[1][5][]5}}
\newlabel{tab:strat}{{2a}{5}{\textbf {Strategy for learning scale equivariance.}\caption@thelabel \relax }{table.caption.4}{}}
\newlabel{tab:strat@cref}{{[subtable][1][2]2a}{[1][5][]5}}
\newlabel{sub@tab:strat}{{a}{5}{\textbf {Strategy for learning scale equivariance.}\caption@thelabel \relax }{table.caption.4}{}}
\newlabel{sub@tab:strat@cref}{{[subtable][1][2]2a}{[1][5][]5}}
\newlabel{tab:feat_scale}{{2b}{5}{\textbf {Scales of input feature map.}\caption@thelabel \relax }{table.caption.4}{}}
\newlabel{tab:feat_scale@cref}{{[subtable][2][2]2b}{[1][5][]5}}
\newlabel{sub@tab:feat_scale}{{b}{5}{\textbf {Scales of input feature map.}\caption@thelabel \relax }{table.caption.4}{}}
\newlabel{sub@tab:feat_scale@cref}{{[subtable][2][2]2b}{[1][5][]5}}
\newlabel{tab:window_size}{{2c}{5}{\textbf {Reference window size.}\caption@thelabel \relax }{table.caption.4}{}}
\newlabel{tab:window_size@cref}{{[subtable][3][2]2c}{[1][5][]5}}
\newlabel{sub@tab:window_size}{{c}{5}{\textbf {Reference window size.}\caption@thelabel \relax }{table.caption.4}{}}
\newlabel{sub@tab:window_size@cref}{{[subtable][3][2]2c}{[1][5][]5}}
\newlabel{tab:num_scale}{{2d}{5}{\textbf {Number of window scales}.\caption@thelabel \relax }{table.caption.4}{}}
\newlabel{tab:num_scale@cref}{{[subtable][4][2]2d}{[1][5][]5}}
\newlabel{sub@tab:num_scale}{{d}{5}{\textbf {Number of window scales}.\caption@thelabel \relax }{table.caption.4}{}}
\newlabel{sub@tab:num_scale@cref}{{[subtable][4][2]2d}{[1][5][]5}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces  \textbf  {Ablation on the design of attention mechanism} using a plain ViT backbone on COCO\nobreakspace  {}\cite  {lin2014mscoco} object detection and instance segmentation. BoxeR\xspace  receives the single-scale input from ViT and makes predictions. Compared to the na\"ive baseline which employs BoxeR and box-attention \cite  {nguyen2022boxer} with single-scale features, BoxeR\xspace  improves performance of the plain detector.\relax }}{5}{table.caption.4}\protected@file@percent }
\newlabel{tab:det_ablation}{{2}{5}{\textbf {Ablation on the design of attention mechanism} using a plain ViT backbone on COCO~\cite {lin2014mscoco} object detection and instance segmentation. \ours receives the single-scale input from ViT and makes predictions. Compared to the na\"ive baseline which employs BoxeR and box-attention \cite {nguyen2022boxer} with single-scale features, \ours improves performance of the plain detector.\relax }{table.caption.4}{}}
\newlabel{tab:det_ablation@cref}{{[table][2][]2}{[1][5][]5}}
\@writefile{brf}{\backcite{lin2014mscoco}{{5}{2}{table.caption.4}}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{5}{2}{table.caption.4}}}
\@writefile{brf}{\backcite{lin2014mscoco}{{5}{4}{table.caption.4}}}
\citation{nguyen2022boxer}
\citation{lin2014mscoco}
\citation{lin2014mscoco}
\citation{cai2019cascadercnn}
\citation{li2022vitdet}
\citation{zhu2021deformable,cheng2022mask2former}
\citation{chen2022uvit}
\citation{lin2014mscoco}
\citation{lin2014mscoco}
\citation{nguyen2022boxer}
\citation{nicolas2020detr}
\citation{nguyen2022boxer}
\@writefile{brf}{\backcite{nguyen2022boxer}{{6}{\caption@xref {??}{ on input line 150}}{table.caption.5}}}
\newlabel{tab:attn}{{3a}{6}{{\bf Instance-attention \vs Masked instance-attention.} \caption@thelabel \relax }{table.caption.5}{}}
\newlabel{tab:attn@cref}{{[subtable][1][3]3a}{[1][6][]6}}
\newlabel{sub@tab:attn}{{a}{6}{{\bf Instance-attention \vs Masked instance-attention.} \caption@thelabel \relax }{table.caption.5}{}}
\newlabel{sub@tab:attn@cref}{{[subtable][1][3]3a}{[1][6][]6}}
\newlabel{tab:pdecoder}{{3b}{6}{{\bf Number of high-resolution layers.}\caption@thelabel \relax }{table.caption.5}{}}
\newlabel{tab:pdecoder@cref}{{[subtable][2][3]3b}{[1][6][]6}}
\newlabel{sub@tab:pdecoder}{{b}{6}{{\bf Number of high-resolution layers.}\caption@thelabel \relax }{table.caption.5}{}}
\newlabel{sub@tab:pdecoder@cref}{{[subtable][2][3]3b}{[1][6][]6}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces  \textbf  {Ablation on mask prediction adaptation} in a plain detector on COCO\nobreakspace  {}\cite  {lin2014mscoco} panoptic segmentation. The masked instance-attention along with high-resolution layers improve the performance of BoxeR\xspace  on panoptic segmentation, showing its effectiveness in capturing fine-grained information.\relax }}{6}{table.caption.5}\protected@file@percent }
\newlabel{tab:pan_ablation}{{3}{6}{\textbf {Ablation on mask prediction adaptation} in a plain detector on COCO~\cite {lin2014mscoco} panoptic segmentation. The masked instance-attention along with high-resolution layers improve the performance of \ours on panoptic segmentation, showing its effectiveness in capturing fine-grained information.\relax }{table.caption.5}{}}
\newlabel{tab:pan_ablation@cref}{{[table][3][]3}{[1][6][]6}}
\@writefile{brf}{\backcite{lin2014mscoco}{{6}{3}{table.caption.5}}}
\@writefile{brf}{\backcite{he2022mae}{{6}{4}{table.caption.4}}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{6}{4}{table.caption.4}}}
\@writefile{brf}{\backcite{loshchilov2019adamw}{{6}{4}{table.caption.4}}}
\@writefile{brf}{\backcite{dosovitskiy2021vit}{{6}{4}{table.caption.4}}}
\@writefile{brf}{\backcite{shiasi2021lsjitter}{{6}{4}{table.caption.4}}}
\@writefile{brf}{\backcite{shiasi2021lsjitter}{{6}{4}{table.caption.4}}}
\@writefile{brf}{\backcite{li2022vitdet}{{6}{4}{table.caption.4}}}
\@writefile{brf}{\backcite{cai2019cascadercnn}{{6}{4}{table.caption.5}}}
\@writefile{brf}{\backcite{li2022vitdet}{{6}{4}{table.caption.5}}}
\@writefile{brf}{\backcite{zhu2021deformable,cheng2022mask2former}{{6}{4}{table.caption.5}}}
\@writefile{brf}{\backcite{chen2022uvit}{{6}{4}{table.caption.5}}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{6}{4}{figure.caption.6}}}
\@writefile{brf}{\backcite{nicolas2020detr}{{6}{4}{figure.caption.6}}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Qualitative results} for object detection and panoptic segmentation on the COCO\nobreakspace  {}\cite  {lin2014mscoco} 2017 {\texttt  {val}}\xspace  set generated by BoxeR\xspace  . Note that BoxeR\xspace  gives good predictions on small objects.\relax }}{6}{figure.caption.6}\protected@file@percent }
\@writefile{brf}{\backcite{lin2014mscoco}{{6}{3}{figure.caption.6}}}
\newlabel{fig:vis}{{3}{6}{\textbf {Qualitative results} for object detection and panoptic segmentation on the COCO~\cite {lin2014mscoco} 2017 \val set generated by \ours . Note that \ours gives good predictions on small objects.\relax }{figure.caption.6}{}}
\newlabel{fig:vis@cref}{{[figure][3][]3}{[1][6][]6}}
\citation{liu2021swintransformer}
\citation{fan2021mvit}
\citation{cheng2021maskformer}
\citation{cheng2022mask2former}
\citation{li2022vitdet}
\citation{nguyen2022boxer}
\citation{chen2022uvit}
\citation{lin2014mscoco}
\citation{li2022vitdet}
\citation{lin2014mscoco}
\citation{li2022vitdet}
\citation{li2022vitdet}
\citation{nguyen2022boxer}
\citation{cheng2022mask2former}
\bibstyle{ieee_fullname}
\bibdata{ddn}
\@writefile{brf}{\backcite{liu2021swintransformer}{{7}{\caption@xref {??}{ on input line 260}}{table.caption.7}}}
\@writefile{brf}{\backcite{fan2021mvit}{{7}{\caption@xref {??}{ on input line 261}}{table.caption.7}}}
\@writefile{brf}{\backcite{cheng2021maskformer}{{7}{\caption@xref {??}{ on input line 262}}{table.caption.7}}}
\@writefile{brf}{\backcite{cheng2022mask2former}{{7}{\caption@xref {??}{ on input line 263}}{table.caption.7}}}
\@writefile{brf}{\backcite{li2022vitdet}{{7}{\caption@xref {??}{ on input line 266}}{table.caption.7}}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{7}{\caption@xref {??}{ on input line 267}}{table.caption.7}}}
\@writefile{brf}{\backcite{chen2022uvit}{{7}{\caption@xref {??}{ on input line 270}}{table.caption.7}}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces \textbf  { Universal detection and segmentation comparison} for object detection, instance segmentation, and panoptic segmentation on the COCO\nobreakspace  {}\cite  {lin2014mscoco} {\texttt  {val}}\xspace  set. By default, all backbones are pre-trained using ImageNet1K. ($\dag  $: backbones pre-trained on ImageNet-22K; $\ddag  $: BoxeR with SimpleFPN from \cite  {li2022vitdet}; $5\times $: methods fine-tuned using $5\times $ schedule; n/a: method is not applicable to the task). Methods in \textcolor {gray}{gray color} are with a convolution-based detection head. BoxeR\xspace  as a plain detector demonstrates competitive performance compared to many specialized or end-to-end models with a small amount of computation and is the only one suited for all three tasks.\relax }}{7}{table.caption.7}\protected@file@percent }
\@writefile{brf}{\backcite{lin2014mscoco}{{7}{4}{table.caption.7}}}
\@writefile{brf}{\backcite{li2022vitdet}{{7}{4}{table.caption.7}}}
\newlabel{tab:det_main}{{4}{7}{\textbf { Universal detection and segmentation comparison} for object detection, instance segmentation, and panoptic segmentation on the COCO~\cite {lin2014mscoco} \val set. By default, all backbones are pre-trained using ImageNet1K. ($\dag $: backbones pre-trained on ImageNet-22K; $\ddag $: BoxeR with SimpleFPN from \cite {li2022vitdet}; $5\times $: methods fine-tuned using $5\times $ schedule; n/a: method is not applicable to the task). Methods in \textcolor {gray}{gray color} are with a convolution-based detection head. \ours as a plain detector demonstrates competitive performance compared to many specialized or end-to-end models with a small amount of computation and is the only one suited for all three tasks.\relax }{table.caption.7}{}}
\newlabel{tab:det_main@cref}{{[table][4][]4}{[1][7][]7}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{7}{4}{figure.caption.6}}}
\@writefile{brf}{\backcite{li2022vitdet}{{7}{4}{table.caption.7}}}
\@writefile{brf}{\backcite{nguyen2022boxer}{{7}{4}{table.caption.7}}}
\@writefile{brf}{\backcite{cheng2022mask2former}{{7}{4}{table.caption.7}}}
\@writefile{toc}{\contentsline {section}{\numberline {5}\hskip -1em.\nobreakspace  {}Conclusion and Limitations}{7}{section.5}\protected@file@percent }
\newlabel{sec:conclusion}{{5}{7}{\hskip -1em.~Conclusion and Limitations}{section.5}{}}
\newlabel{sec:conclusion@cref}{{[section][5][]5}{[1][7][]7}}
\gdef \@abspage@last{7}
