\section{Background}
\label{sec:background}
Our goal is to further simplify the detection and segmentation pipeline from~\cite{zhu2021deformable,li2022vitdet,nguyen2022boxer}, and to prove the effectiveness of the plain detector in object detection and segmentation tasks. To do so, we focus on the recent progress in end-to-end detection and segmentation. As a result, we utilize the sparse attention mechanism, Box-Attention in~\cite{nguyen2022boxer} and Deformable Attention in~\cite{zhu2021deformable}, as strong baselines due to its effectiveness in learning discriminative object representations while being lightweight in computation.

% Given the input feature map from the backbone, the encoder layers with the sparse attention mechanism will output contextual representations. The contextual representations are utilized to predict object proposals and to initialize object queries for the decoder. We denote the input feature map of an encoder layer as $e \in \mathbb{R}^{H_e \times W_e \times d}$ and the query vector $q \in \mathbb{R}^d$, with $H_e, W_e, d$ denoting height, width, and dimension of the input features respectively. 

\paragraph{Multi-head Box-Attention.} In box-attention, each query vector $q \in \mathbb{R}^d$ in the input feature map is assigned a reference window $r {=} [x, y, w, h]$, where $x, y$ indicate the query coordinate and $w, h$ are the size of the reference window both being normalized by the image size. The box-attention refines the reference window into a region of interest, $r'$, as:
%
\begin{equation}
    r' = F_\text{scale}\big(F_\text{translate}(r, q), q\big), 
\end{equation}
\begin{equation}
    F_\text{scale}(r, q) = [x, y, w + \Delta_w, h + \Delta_h], 
\end{equation}
\begin{equation}
    F_\text{translate}(r, q) = [x + \Delta_x, y + \Delta_y, w, h],
\end{equation}
%
where $F_\text{scale}$ and $F_\text{translate}$ are the scaling and translation transformations, $\Delta_x, \Delta_y, \Delta_w$ and $\Delta_h$ are the offsets regarding to the reference window $r$. A linear projection ($\mathbb{R}^d \rightarrow \mathbb{R}^{4}$) is applied on $q$ to predict offset parameters (\ie, $\Delta_x, \Delta_y, \Delta_w$ and $\Delta_h$) \wrt the window size.

During the $i$-th attention head computation, a $2 {\times} 2$ feature grid is sampled from the corresponding region of interest $r'_i$, resulting in a set of value features $v_i \in \mathbb{R}^{2 \times 2 \times d_h}$. The $2 {\times} 2$ attention scores are efficiently generated by computing a dot-product between $q \in \mathbb{R}^d$ and relative position embeddings ($k_i \in \mathbb{R}^{2 \times 2 \times d}$) followed by a $\softmax$ function. The attended feature $\mathrm{head}_i \in \mathbb{R}^{d_h}$ is a weighted sum of $2 {\times} 2$ value features in $v_i$ with the corresponding attention weights:
%
\begin{equation}
    \alpha =  \softmax(q^{\top} {k_i}),
\end{equation}
\begin{equation}
    \mathrm{head}_i = \mathop{\mathrm{\boxattnop}}(q, k_i, v_i) = \sum_{j=0}^{2 \times 2} \alpha_j v_{i_j},
\end{equation}
%
where $q \in \mathbb{R}^d$, $k_i \in \mathbb{R}^{2 \times 2 \times d}$, $v_i \in \mathbb{R}^{2 \times 2 \times d_h}$ are query, key and value vectors of box-attention, $\alpha_j$ is the $j$-th attention weight, and $v_{i_j}$ is the $j$-th feature vector in the feature grid $v_i$. To better capture objects at different scales, the box-attention~\cite{nguyen2022boxer} takes $t$ multi-scale feature maps, $\{e^j\}_{j=1}^t$, as its inputs in order to produce $\mathrm{head}_i$.

\paragraph{Multi-head Deformable Attention.} In deformable attention, each query vector $q \in \mathbb{R}^d$ is assigned a reference point $p=[x,y]$, where $x,y$ indicate the query coordinate normalized by the image size. The deformable attention attends to points of interest around the query coordinate, $p'$, as:
%
\begin{equation}
    p' = F_\text{deform}(p, q) = [x + \Delta_x, y + \Delta_y],
\end{equation}
%
where $F_\text{deform}$ is the deformable function, $\Delta_x,\Delta_y$ are the offets regarding to the reference point $p$. Similarly, a linear projection ($\mathbb{R}^d \rightarrow \mathbb{R}^2$) is applied on $q$ to predict offset paramters \wrt the reference point.

During the $i$-th attention head computation, the deformable attention predicts a set of 4 points, resulting in value features $v_i \in \mathbb{R}^{4\times d_h}$. The $4$ attention scores are generated by applying another linear projection, $g_i: \mathbb{R} \rightarrow \mathbb{R}^4$, on $q$ followed by a $\softmax$ function. The attended feature $\mathrm{head}_i \in \mathbb{R}^{d_h}$ is a weighted sum of $4$ value features in $v_i$ with the corresponding attention weights:
%
\begin{equation}
    \alpha =  \softmax\big(g_i(q)\big),
\end{equation}
\begin{equation}
    \mathrm{head}_i = \mathop{\mathrm{\deformattnop}}(q, v_i) = \sum_{j=0}^{4} \alpha_j v_{i_j},
\end{equation}
%
where $q \in \mathbb{R}^d$, $v_i \in \mathbb{R}^{2 \times 2 \times d_h}$ are query, value vectors of deformable attention, $\alpha_j$ is the $j$-th attention weight, and $v_{i_j}$ is the $j$-th feature vector in the value $v_i$. The deformable attention~\cite{zhu2021deformable} also utilizes $t$ multi-scale feature maps, $\{e^j\}_{j=1}^t$, as its inputs in order to produce $\mathrm{head}_i$.


% Similar to self-attention~\cite{vaswani2017transformer}, box-attention aggregates $n$ multi-head features from regions of interest:
% %
% \begin{equation}
% \label{eq:multihead}
%     \mathop{\mathrm{MultiHeadAttention}} = \mathop{\mathrm{Concat}}(\mathrm{head}_1, \ldots, \mathrm{head}_n) \, W^O,
% \end{equation}
% %

The sparse attention like box-attention and deformable attention lies at the core of recent end-to-end detection and segmentation models due to its ability of capturing object information with lower complexity. The effectiveness and efficiency of these attention mechanisms bring up the question: \textit{Is multi-scale object information learnable within the detector which is non-hierarchical and single-scale?}
