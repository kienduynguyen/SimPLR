\section{Background}
\label{sec:background}
Our goal is to further simplify the detection and segmentation pipeline from~\cite{zhu2021deformable,li2022vitdet,nguyen2022boxer}, and to prove the effectiveness of the plain detector in object detection and segmentation tasks. To do so, we focus on the recent progress in end-to-end detection and segmentation. Specifically, we utilize the box-attention mechanism by~\cite{nguyen2022boxer} as a strong baseline due to its effectiveness in learning discriminative object representations while being lightweight in computation. 

Given the input feature map from the backbone, the encoder layers with box-attention will output contextual representations. The contextual representations are utilized to predict object proposals and to initialize object queries for the decoder. We denote the input feature map of an encoder layer as $e \in \mathbb{R}^{H_e \times W_e \times d}$ and the query vector $q \in \mathbb{R}^d$, with $H_e, W_e, d$ denoting height, width, and dimension of the input features respectively. Each query vector $q \in \mathbb{R}^d$ in the input feature map is assigned a reference window $r {=} [x, y, w, h]$, where $x, y$ indicate the query coordinate and $w, h$ are the size of the reference window both being normalized by the image size. The box-attention refines the reference window into a region of interest, $r'$, as:
%
\begin{equation}
    r' = F_\text{scale}\big(F_\text{translate}(r, q), q\big), 
\end{equation}
\begin{equation}
    F_\text{scale}(r, q) = [x, y, w + \Delta_w, h + \Delta_h], 
\end{equation}
\begin{equation}
    F_\text{translate}(r, q) = [x + \Delta_x, y + \Delta_y, w, h],
\end{equation}
%
where $F_\text{scale}$ and $F_\text{translate}$ are the scaling and translation transformations, $\Delta_x, \Delta_y, \Delta_w$ and $\Delta_h$ are the offsets regarding to the reference window $r$. A linear projection ($\mathbb{R}^d \rightarrow \mathbb{R}^{4}$) is applied on $q$ to predict offset parameters (\ie, $\Delta_x, \Delta_y, \Delta_w$ and $\Delta_h$) \wrt the window size.

Similar to self-attention~\cite{vaswani2017transformer}, box-attention aggregates $n$ multi-head features from regions of interest:
%
\begin{equation}
\label{eq:multihead}
    \mathop{\mathrm{MultiHeadAttention}} = \mathop{\mathrm{Concat}}(\mathrm{head}_1, \ldots, \mathrm{head}_n) \, W^O,
\end{equation}
%
During the $i$-th attention head computation, a $2 {\times} 2$ feature grid is sampled from the corresponding region of interest $r'_i$, resulting in a set of value features $v_i \in \mathbb{R}^{2 \times 2 \times d_h}$. The $2 {\times} 2$ attention scores are efficiently generated by computing a dot-product between $q \in \mathbb{R}^d$ and relative position embeddings ($k_i \in \mathbb{R}^{2 \times 2 \times d}$) followed by a $\softmax$ function. The attended feature $\mathrm{head}_i \in \mathbb{R}^{d_h}$ is a weighted average of the $2 {\times} 2$ value features in $v_i$ with the corresponding attention weights:
%
\begin{equation}
    \alpha =  \softmax(q^{\top} {k_i}),
\end{equation}
\begin{equation}
    \mathrm{head}_i = \mathop{\mathrm{\boxattnop}}(q, k_i, v_i) = \sum_{j=0}^{2 \times 2} \alpha_j v_{i_j},
\end{equation}
%
where $q \in \mathbb{R}^d$, $k_i \in \mathbb{R}^{2 \times 2 \times d}$, $v_i \in \mathbb{R}^{2 \times 2 \times d_h}$ are query, key and value vectors of box-attention, $\alpha_j$ is the $j$-th attention weight, and $v_{i_j}$ is the $j$-th feature vector in the feature grid $v_i$. To better capture objects at different scales, the box-attention~\cite{nguyen2022boxer} takes $t$ multi-scale feature maps, $\{e^j\}_{j=1}^t$, as its inputs. In the $i$-th attention, $t$ feature grids are sampled from each of multi-scale feature maps in order to produce $\mathrm{head}_i$.

By computing the attended feature within regions of interest in each attention head, box-attention shows strong performance in object detection and instance segmentation with a small computational budget. The transformation functions (\ie, translation and scaling) allow box-attention to capture long-range dependencies. The effectiveness and efficiency of box-attention mechanism brings up the question: \textit{Is multi-scale object information learnable within a single-scale feature map?}
