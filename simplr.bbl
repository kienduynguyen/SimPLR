\begin{thebibliography}{10}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }
\providecommand{\doi}[1]{https://doi.org/#1}

\bibitem{bao2022beit}
Bao, H., Dong, L., Piao, S., Wei, F.: Beit: Bert pre-training of image transformers. In: ICLR (2022)

\bibitem{brown2020gpt3}
Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Amanda~Askell, S.A., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language models are few-shot learners. In: NeurIPS (2020)

\bibitem{nicolas2020detr}
Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-to-end object detection with transformers. In: ECCV (2020)

\bibitem{chen2020igpt}
Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Dhariwal, P., Luan, D., Sutskever, I.: Generative pretraining from pixels. In: ICML (2020)

\bibitem{chen2020simclr}
Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for contrastive learning of visual representations. In: ICML (2020)

\bibitem{chen2022uvit}
Chen, W., Du, X., Yang, F., Beyer, L., Zhai, X., Lin, T.Y., Chen, H., Li, J., Song, X., Wang, Z., Zhou, D.: A simple single-scale vision transformer for object localization and instance segmentation. In: ECCV (2022)

\bibitem{cheng2022mask2former}
Cheng, B., Misra, I., Schwing, A.G., Kirillov, A., Girdhar, R.: Mask2former: Masked-attention mask transformer for universal image segmentation. In: CVPR (2022)

\bibitem{cheng2021maskformer}
Cheng, B., Schwing, A.G., Kirillov, A.: Per-pixel classification is not all you need for semantic segmentation. In: NeurIPS (2021)

\bibitem{dehghani2023scalingvit22b}
Dehghani, M., Djolonga, J., Mustafa, B., et~al.: Scaling vision transformers to 22 billion parameters. In: ICML (2023)

\bibitem{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: CVPR (2009)

\bibitem{devlin2019bert}
Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirectional transformers for language understanding. In: ACL (2019)

\bibitem{dong2021solq}
Dong, B., Zeng, F., Wang, T., Zhang, X., Wei, Y.: {SOLQ}: Segmenting objects by learning queries. In: NeurIPS (2021)

\bibitem{dosovitskiy2021vit}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is worth 16x16 words: Transformers for image recognition at scale. In: ICLR (2021)

\bibitem{fan2021mvit}
Fan, H., Xiong, B., Mangalam, K., Li, Y., Yan, Z., Malik, J., Feichtenhofer, C.: Multiscale vision transformers. In: ICCV (2021)

\bibitem{shiasi2021lsjitter}
Ghiasi, G., Cui, Y., Srinivas, A., Qian, R., Lin, T., Cubuk, E.D., Le, Q.V., Zoph, B.: Simple copy-paste is a strong data augmentation method for instance segmentation. In: CVPR (2021)

\bibitem{girshick2014rcnn}
Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accurate object detection and semantic segmentation. In: CVPR (2014)

\bibitem{he2022mae}
He, K., Chen, X., Xie, S., Li, Y., Doll{\'a}r, P., Girshick, R.: Masked autoencoders are scalable vision learners. In: CVPR (2022)

\bibitem{he2017maskrcnn}
He, K., Gkioxari, G., Dollár, P., Girshick, R.: Mask {R-CNN}. In: ICCV (2017)

\bibitem{kaiming2016resnet}
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR (2016)

\bibitem{heo2021rethinkingvit}
Heo, B., Yun, S., Han, D., Chun, S., Choe, J., Oh, S.J.: Rethinking spatial dimensions of vision transformers. In: ICCV (2021)

\bibitem{huang2017densenet}
Huang, G., Liu, Z., van~der Maaten, L., Weinberger, K.Q.: Densely connected convolutional networks. In: CVPR (2017)

\bibitem{jia2023hybridmatching}
Jia, D., Yuan, Y., He, H., Wu, X., Yu, H., Lin, W., Sun, L., Zhang, C., Hu, H.: Detrs with hybrid matching. In: CVPR (2023)

\bibitem{kirillov2019panoptic}
Kirillov, A., He, K., Girshick, R., Rother, C., Dollar, P.: Panoptic segmentation. In: CVPR (2019)

\bibitem{lecun95convolutional}
LeCun, Y., Bengio, Y.: Convolutional Networks for Images, Speech and Time Series, pp. 255--258. MIT Press (1995)

\bibitem{li2023maskdino}
Li, F., Zhang, H., xu, H., Liu, S., Zhang, L., Ni, L.M., Shum, H.Y.: Mask dino: Towards a unified transformer-based framework for object detection and segmentation. In: CVPR (2023)

\bibitem{li2022vitdet}
Li, Y., Mao, H., Girshick, R., He, K.: Exploring plain vision transformer backbones for object detection. In: ECCV (2022)

\bibitem{tsung2017fpn}
{Lin}, T.Y., {Dollár}, P., {Girshick}, R., {He}, K., {Hariharan}, B., {Belongie}, S.: Feature pyramid networks for object detection. In: CVPR (2017)

\bibitem{lin2014mscoco}
Lin, T., Maire, M., Belongie, S.J., Bourdev, L.D., Girshick, R.B., Hays, J., Perona, P., Ramanan, D., Doll{\'{a}}r, P., Zitnick, C.L.: Microsoft {COCO:} common objects in context. In: ECCV (2014)

\bibitem{lin2023plaindetr}
Lin, Y., Yuan, Y., Zhang, Z., Li, C., Zheng, N., Hu, H.: {DETR} does not need multi-scale or locality design. In: ICCV (2023)

\bibitem{wei2016ssd}
Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.Y., Berg, A.C.: {SSD}: Single shot multibox detector. In: ECCV (2016)

\bibitem{liu2021swintransformer}
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: ICCV (2021)

\bibitem{loshchilov2019adamw}
Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: ICLR (2019)

\bibitem{nguyen2022boxer}
Nguyen, D., Ju, J., Booij, O., Oswald, M.R., Snoek, C.G.M.: Boxer: Box-attention for 2d and 3d transformers. In: CVPR (2022)

\bibitem{peng2022beitv2}
Peng, Z., Dong, L., Bao, H., Ye, Q., Wei, F.: {BEiT v2}: Masked image modeling with vector-quantized visual tokenizers. arXiv preprint arXiv:2208.06366  (2022)

\bibitem{ren2015faster_rcnn}
Ren, S., He, K., Girshick, R., Sun, J.: Faster {R-CNN}: Towards real-time object detection with region proposal networks. In: NeurIPS (2015)

\bibitem{simonyan2015vgg}
Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. In: ICLR (2015)

\bibitem{touvron2021deit}
Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., Jegou, H.: Training data-efficient image transformers and distillation through attention. In: International Conference on Machine Learning (2021)

\bibitem{touvron2022deit3}
Touvron, H., Cord, M., Jegou, H.: Deit iii: Revenge of the vit. arXiv preprint arXiv:2204.07118  (2022)

\bibitem{vaswani2017transformer}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. In: NeurIPS (2017)

\bibitem{wang2021maxdeeplab}
Wang, H., Zhu, Y., Adam, H., Yuille, A., Chen, L.C.: Max-deeplab: End-to-end panoptic segmentation with mask transformers. In: CVPR (2021)

\bibitem{wang2021pvit}
Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In: ICCV (2021)

\bibitem{xie2017resnext}
Xie, S., Girshick, R.B., Doll{\'{a}}r, P., Tu, Z., He, K.: Aggregated residual transformations for deep neural networks. In: CVPR (2017)

\bibitem{yu2022kmmask}
Yu, Q., Wang, H., Qiao, S., Collins, M., Zhu, Y., Adam, H., Yuille, A., Chen, L.C.: k-means mask transformer. In: ECCV (2022)

\bibitem{zhai2022scalingvit}
Zhai, X., Kolesnikov, A., Houlsby, N., Beyer, L.: Scaling vision transformers. In: CVPR (2022)

\bibitem{zhang2023dino}
Zhang, H., Li, F., Liu, S., Zhang, L., Su, H., Zhu, J., Ni, L., Shum, H.Y.: Dino: Detr with improved denoising anchor boxes for end-to-end object detection. In: ICLR (2023)

\bibitem{zhang2021knet}
Zhang, W., Pang, J., Chen, K., Loy, C.C.: K-net: Towards unified image segmentation. In: NeurIPS (2021)

\bibitem{zhu2021deformable}
Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable {DETR}: Deformable transformers for end-to-end object detection. In: ICLR (2021)

\end{thebibliography}
