\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{li2022vitdet}
\citation{li2022vitdet}
\citation{li2022vitdet}
\citation{wu2018groupnorm}
\citation{nguyen2022boxer}
\citation{he2017maskrcnn}
\citation{cheng2022mask2former}
\@writefile{toc}{\contentsline {section}{\numberline {A}Implementation Details}{1}{appendix.A}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {A}{\ignorespaces  \textbf  {The creation of input features. Left:} The creation of feature pyramids from the last feature of the plain backbone, ViT, in SimpleFPN\nobreakspace  {}\citep  {li2022vitdet} where different stacks of convolutional layers are used to create features at different scales. \textbf  {Right:} The design of our single-scale feature map with only one layer. \relax }}{1}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:fpn_vs_ss}{{A}{1}{\textbf {The creation of input features. Left:} The creation of feature pyramids from the last feature of the plain backbone, ViT, in SimpleFPN~\citep {li2022vitdet} where different stacks of convolutional layers are used to create features at different scales. \textbf {Right:} The design of our single-scale feature map with only one layer. \relax }{figure.caption.1}{}}
\newlabel{fig:fpn_vs_ss@cref}{{[figure][1][2147483647]A}{[1][1][]1}}
\citation{nguyen2022boxer}
\citation{nguyen2022boxer}
\citation{nguyen2022boxer}
\citation{nguyen2022boxer}
\citation{nguyen2022boxer}
\citation{nguyen2022boxer}
\citation{nguyen2022boxer}
\citation{lin2017focalloss}
\citation{milletari2016vnet}
\citation{rezatofighi2019giou}
\citation{nguyen2022boxer}
\citation{li2022vitdet}
\citation{touvron2021deit3}
\citation{touvron2022deit3}
\citation{nguyen2022boxer}
\citation{nguyen2022boxer}
\@writefile{lof}{\contentsline {figure}{\numberline {B}{\ignorespaces  \textbf  {Masked Instance-Attention. Left:} The box-attention\nobreakspace  {}\citep  {nguyen2022boxer} which samples $2\times 2$ grid features in the region of interest. \textbf  {Right:} Our masked instance-attention for dense grid sampling that employs masking strategy to capture object boundary. The $2\times 2$ attention scores are denoted in four colours and the masked attention score is shown in white.\relax }}{2}{figure.caption.2}\protected@file@percent }
\newlabel{fig:masked_instance_attn}{{B}{2}{\textbf {Masked Instance-Attention. Left:} The box-attention~\citep {nguyen2022boxer} which samples $2\times 2$ grid features in the region of interest. \textbf {Right:} Our masked instance-attention for dense grid sampling that employs masking strategy to capture object boundary. The $2\times 2$ attention scores are denoted in four colours and the masked attention score is shown in white.\relax }{figure.caption.2}{}}
\newlabel{fig:masked_instance_attn@cref}{{[figure][2][2147483647]B}{[1][2][]2}}
\@writefile{lot}{\contentsline {table}{\numberline {A}{\ignorespaces Hyper-parameters of backbone and detection head for different sizes of SimPLR\xspace  (base -- large -- huge models). Note that these settings are the same for all three tasks.\relax }}{2}{table.caption.3}\protected@file@percent }
\newlabel{tab:hyper}{{A}{2}{Hyper-parameters of backbone and detection head for different sizes of \ours (base -- large -- huge models). Note that these settings are the same for all three tasks.\relax }{table.caption.3}{}}
\newlabel{tab:hyper@cref}{{[table][1][2147483647]A}{[1][2][]2}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Additional Results}{2}{appendix.B}\protected@file@percent }
\citation{deng2009imagenet}
\citation{lin2014mscoco}
\bibdata{simplr}
\bibcite{cheng2022mask2former}{{1}{2022}{{Cheng et~al.}}{{Cheng, Misra, Schwing, Kirillov, and Girdhar}}}
\bibcite{deng2009imagenet}{{2}{2009}{{Deng et~al.}}{{Deng, Dong, Socher, Li, Li, and Fei-Fei}}}
\bibcite{he2017maskrcnn}{{3}{2017}{{He et~al.}}{{He, Gkioxari, Doll√°r, and Girshick}}}
\bibcite{li2022vitdet}{{4}{2022}{{Li et~al.}}{{Li, Mao, Girshick, and He}}}
\bibcite{lin2014mscoco}{{5}{2014}{{Lin et~al.}}{{Lin, Maire, Belongie, Bourdev, Girshick, Hays, Perona, Ramanan, Doll{\'{a}}r, and Zitnick}}}
\bibcite{lin2017focalloss}{{6}{2017}{{Lin et~al.}}{{Lin, Goyal, Girshick, He, and Doll{\'{a}}r}}}
\bibcite{milletari2016vnet}{{7}{2016}{{Milletari et~al.}}{{Milletari, Navab, and Ahmadi}}}
\@writefile{lot}{\contentsline {table}{\numberline {B}{\ignorespaces \textbf  {More panoptic segmentation comparison} between SimPLR\xspace  with ViT-B backbone pre-trained using MAE and other methods with Swin-B backbone. All backbones are pre-trained on ImageNet-1K. SimPLR\xspace  still shows competitive results when using only single-scale input.\relax }}{3}{table.caption.4}\protected@file@percent }
\newlabel{tab:more_panop}{{B}{3}{\textbf {More panoptic segmentation comparison} between \ours with ViT-B backbone pre-trained using MAE and other methods with Swin-B backbone. All backbones are pre-trained on ImageNet-1K. \ours still shows competitive results when using only single-scale input.\relax }{table.caption.4}{}}
\newlabel{tab:more_panop@cref}{{[table][2][2147483647]B}{[1][2][]3}}
\@writefile{lot}{\contentsline {table}{\numberline {C}{\ignorespaces  \textbf  {Ablation on pre-training strategies} of the plain ViT backbone using SimPLR\xspace  evaluated on COCO object detection and instance segmentation. We compare the ViT backbone pre-trained using supervised methods (\emph  {top} row) \emph  {vs}\onedot  self-supervised methods (\emph  {bottom} row) with different sizes of pre-training dataset (ImageNet-1K \emph  {vs}\onedot  ImageNet-21K). Here, we use the $5\times $ schedule as in \cite  {nguyen2022boxer}. It can be seen that SimPLR\xspace  with the plain ViT backbone benefits from better pre-training approaches and with more pre-training data. \relax }}{3}{table.caption.5}\protected@file@percent }
\newlabel{tab:pretrain}{{C}{3}{\textbf {Ablation on pre-training strategies} of the plain ViT backbone using \ours evaluated on COCO object detection and instance segmentation. We compare the ViT backbone pre-trained using supervised methods (\emph {top} row) \vs self-supervised methods (\emph {bottom} row) with different sizes of pre-training dataset (ImageNet-1K \vs ImageNet-21K). Here, we use the $5\times $ schedule as in \cite {nguyen2022boxer}. It can be seen that \ours with the plain ViT backbone benefits from better pre-training approaches and with more pre-training data. \relax }{table.caption.5}{}}
\newlabel{tab:pretrain@cref}{{[table][3][2147483647]C}{[1][2][]3}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Qualitative results}{3}{appendix.C}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {D}Asset Licenses}{3}{appendix.D}\protected@file@percent }
\bibcite{nguyen2022boxer}{{8}{2022}{{Nguyen et~al.}}{{Nguyen, Ju, Booij, Oswald, and Snoek}}}
\bibcite{rezatofighi2019giou}{{9}{2019}{{Rezatofighi et~al.}}{{Rezatofighi, Tsoi, Gwak, Sadeghian, Reid, and Savarese}}}
\bibcite{touvron2021deit}{{10}{2021}{{Touvron et~al.}}{{Touvron, Cord, Douze, Massa, Sablayrolles, and Jegou}}}
\bibcite{wu2018groupnorm}{{11}{2018}{{Wu \& He}}{{Wu and He}}}
\bibstyle{iclr2024_conference}
\@writefile{lof}{\contentsline {figure}{\numberline {C}{\ignorespaces \textbf  {Qualitative results} for object detection, instance segmentation, and panoptic segmentation generated by SimPLR\xspace  using ViT-B as backbone on the COCO {\texttt  {val}}\xspace  set. In each pair, the left image shows the visualization of object detection and instance segmentation, while the right ones indicates the panoptic segmentation prediction.\relax }}{4}{figure.caption.6}\protected@file@percent }
\newlabel{fig:vis_sup}{{C}{4}{\textbf {Qualitative results} for object detection, instance segmentation, and panoptic segmentation generated by \ours using ViT-B as backbone on the COCO \val set. In each pair, the left image shows the visualization of object detection and instance segmentation, while the right ones indicates the panoptic segmentation prediction.\relax }{figure.caption.6}{}}
\newlabel{fig:vis_sup@cref}{{[figure][3][2147483647]C}{[1][3][]4}}
\gdef \@abspage@last{4}
